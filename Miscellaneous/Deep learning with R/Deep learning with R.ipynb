{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Overview\n",
    "\n",
    "1. Introduction to Deep Learning\n",
    "    + The Course Overview\n",
    "    + Fundamental Concepts in Deep Learning\n",
    "    + Introduction to Artificial Neural Networks\n",
    "    + Classification with Two-Layers Artificial Neural Networks\n",
    "    + Probabilistic Predictions with Two-Layer ANNs\n",
    "2. Working with Neural Network Architectures\n",
    "    + Introduction to Multi-hidden-layer Architectures\n",
    "    + Neural Network Architectures (Continued)\n",
    "    + Neural Network Architectures\n",
    "    + Tuning ANNs Hyper-Parameters and Best Practices\n",
    "3. Advanced Artificial Neural Networks\n",
    "    + The Learning Process\n",
    "    + Optimization Algorithms and Stochastic Gradient Descent\n",
    "    + Backpropagation\n",
    "    + Hyper-Parameters Optimization\n",
    "4. Convolutional Neural Networks\n",
    "    + Introduction to Convolutional Neural Networks\n",
    "    + Introduction to Convolutional Neural Networks (Continued)\n",
    "    + CNNs in R\n",
    "    + Classifying Real-World Images with Pre-Trained Models\n",
    "5. Recurrent Neural Networks\n",
    "    + Introduction to Recurrent Neural Networks \n",
    "    + Introduction to Long Short-Term Memory\n",
    "    + RNNs in R\n",
    "    + Use-Case - Learning How to Spell English Words from Scratch\n",
    "6. Towards Unsupervised and Reinforcement Learning\n",
    "    + Introduction to Unsupervised and Reinforcement Learning\n",
    "    + Autoencoders\n",
    "    + Restricted Boltzmann Machines and Deep Belief Networks\n",
    "    + Reinforcement Learning with ANNs\n",
    "    + Use-Case - Anomaly Detection through Denoising Autoencoders\n",
    "7. Applications of Deep Learning\n",
    "    + Deep Learning for Computer Vision\n",
    "    + Deep Learning for Natural Language Processing\n",
    "    + Deep Learning for Audio Signal Processing\n",
    "    + Deep Learning for Complex Multimodal Tasks\n",
    "    + Other Important Applications of Deep Learning\n",
    "8. Advanced Topics\n",
    "    + A Complete Comparison of Every DL Packages in R\n",
    "    + Debugging Deep Learning Systems\n",
    "    + GPU and MGPU Computing for Deep Learning\n",
    "    + Research Directions and Open Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "+ Basic understanding of Linear Algebra and Calculus \n",
    "+ Basic knowledge of the R programming language \n",
    "+ Anaconda Jupyter with R notebook support \n",
    "+ No neural networks experience required at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Definition and Key Features \n",
    "\n",
    "+ Deep Learning is a branch of machine learning based on a set of algorithms that attempt to model high level and hierarchical representation in data using: \n",
    "    + Deep graph with multiple processing layers \n",
    "    + Multiple linear and non-linear transformations \n",
    "+ Basic assumptions:\n",
    "    + We can learn from the data everything we need to solve the task \n",
    "    + Employing a large number of very simple computational units, we can solve even complex problems\n",
    "    \n",
    "+ Main difference with classical Machine Learning: \n",
    "    + Learn also the best way of represent the data, and not just the mapping function f: x -> y \n",
    "+ Main difference with Representation Learning:\n",
    "    + Additional layers of more abstract features\n",
    "\n",
    "<img src='./resources/MLFeatures.jpeg' alt='MLfeatures' align ='left' width='40%' height='40%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to learn directly from data\n",
    "\n",
    "+ The central problem is then: how to learn directly from data \n",
    "+ The main strategy is to define a mathematical model with parameters 0 = {W1, ... ,Wn} \n",
    "    and a loss function j(0) which encodes the amount of errors the model is doing\n",
    "+ Then we just need to tweak the parameters in order to **minimize this function** \n",
    "    using the best optimization algorithm available \n",
    "+ An example of loss function for the regression problem could be: \n",
    "\n",
    "<img src='./resources/lossfunction.jpeg' alt='lossfunction' align ='left' width='40%' height='40%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ But many different loss function can be designed for the same problem and some could work better than others!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vast World of ANNs \n",
    "\n",
    "Artificial Neural Networks can be conveniently framed in just two main categories: \n",
    "\n",
    "+ Feed-forward Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/anns_feedforward.png' alt='anns_feedforward' align='left' height='30%' width='20%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Recursive Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/anns_recursive.png' alt='anns_recursive' align='left' height='30%' width='20%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ But in the end, they are pretty flexible to many possible variations: \n",
    "    + The learning rule \n",
    "    + The inference rule \n",
    "    + The architecture \n",
    "+ During the following sections we will present the most common DL models but be free to explore new solutions as well \n",
    "\n",
    "### The Perceptron Algorithm\n",
    "\n",
    "Fundamental computational unit at the heart of every Deep Learning model: \n",
    "\n",
    "+ Invented in 1957 by Frank Rosenblatt \n",
    "+ It's a supervised learning algorithm for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/perceptron1.png' alt='perceptron' align='left' height='30%' width='30%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/perceptron2.png' alt='perceptron' align='left' height='40%' width='30%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/ann.png' alt='ann' align='left' height='50%' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron Geometric Interpretation\n",
    "\n",
    "A perceptron can be used for linearly-solvable problems: \n",
    "\n",
    "+ Geometrically what we want to learn are the parameters of a line or an hyper-plan \n",
    "+ We can use this line to separate or to represent our data for future predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/perceptron3.png' alt='perceptron' align='left' height='50%' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/fig.png' alt='fig' align='left' height='30%' width='30%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/fig1.png' alt='fig' align='left' height='30%' width='30%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multilayer Perceptron\n",
    "\n",
    "The Multilayer Perceptron (MCP) is a feedforward ANNs model that maps sets of input data onto a set of appropriate outputs:\n",
    "\n",
    "+ MLP is a universal approximator \n",
    "+ MLP is a modification of the standard perceptron and can distinguish not linearly separable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/perceptron4.png' alt='perceptron' align='left' height='50%' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/ann_hidden.png' alt='ann-hidden' align='left' height='50%' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R packages for Artificial Neural Networks\n",
    "\n",
    "+ There are many packages available for Artificial Neural Networks and Deep Learning \n",
    "    + **In R:** nnet, neuralnet, RSNNS, deepnet, darch, caret, RNN, Autoencoder, RcppDL, MXNetR etc.\n",
    "    + **Outside R:** Caffe, Tensorflow, Theano, Torch, Deeplearning4j, CNTK \n",
    "\n",
    "### The MNIST Dataset\n",
    "\n",
    "+ As our use case, we will try to classify handwritten digits \n",
    "+ The MNIST dataset is a well-know benchmark in the ML and CV community \n",
    "+ You can download part of the entire dataset here: \n",
    "    + [Kaggle : digit recognizer dataset](https://www.kaggle.com/c/digit-recognizer/download/train.csv)    \n",
    "+ Each image is 28x28 pixels grayscale \n",
    "+ Each pixel has a value between 0 and 255, inclusive \n",
    "+ For each image we have a single vector of 784 integer\n",
    "\n",
    "### Neural Network Design\n",
    "\n",
    "If we want to classify correctly the 10 digits we have to design our architecture first:\n",
    "+ |x| = 784 \n",
    "+ |h| = 5\n",
    "+ |o| = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recover Class Probabilities from an Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ So far In our MLP, we just took the maximum value from the output neurons to decided which class to pick \n",
    "+ We have two main possibilities: \n",
    "    + Normalize the raw output values to sum to one \n",
    "    + Use the Softmax function\n",
    "    \n",
    "### Probability Normalization\n",
    "\n",
    "For normalizing the raw output values we can simply divide each of them by their sum:\n",
    "\n",
    "<img src='./resources/prob_norm.png' alt='prob_norm' align='left' height='10%' width='10%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "Or we can use the Softmax function: \n",
    "\n",
    "<img src='./resources/fn_softmax.png' alt='fn_softmax' align='left' height='15%' width='15%' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "#install.packages('nnet', repos = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "# install.packages('caret', repos = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    }
   ],
   "source": [
    "# Loading the libraries\n",
    "library(nnet)\n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general parameters\n",
    "options(digits = 3)\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "digits_data <- read.csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>42000</li>\n",
       "\t<li>785</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 42000\n",
       "\\item 785\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 42000\n",
       "2. 785\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 42000   785"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'label'</li>\n",
       "\t<li>'pixel0'</li>\n",
       "\t<li>'pixel1'</li>\n",
       "\t<li>'pixel2'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'label'\n",
       "\\item 'pixel0'\n",
       "\\item 'pixel1'\n",
       "\\item 'pixel2'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'label'\n",
       "2. 'pixel0'\n",
       "3. 'pixel1'\n",
       "4. 'pixel2'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"label\"  \"pixel0\" \"pixel1\" \"pixel2\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'pixel780'</li>\n",
       "\t<li>'pixel781'</li>\n",
       "\t<li>'pixel782'</li>\n",
       "\t<li>'pixel783'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'pixel780'\n",
       "\\item 'pixel781'\n",
       "\\item 'pixel782'\n",
       "\\item 'pixel783'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'pixel780'\n",
       "2. 'pixel781'\n",
       "3. 'pixel782'\n",
       "4. 'pixel783'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"pixel780\" \"pixel781\" \"pixel782\" \"pixel783\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>label</th><th scope=col>pixel0</th><th scope=col>pixel1</th><th scope=col>pixel2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " label & pixel0 & pixel1 & pixel2\\\\\n",
       "\\hline\n",
       "\t 1 & 0 & 0 & 0\\\\\n",
       "\t 0 & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "label | pixel0 | pixel1 | pixel2 | \n",
       "|---|---|\n",
       "| 1 | 0 | 0 | 0 | \n",
       "| 0 | 0 | 0 | 0 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  label pixel0 pixel1 pixel2\n",
       "1 1     0      0      0     \n",
       "2 0     0      0      0     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking metadata about data\n",
    "dim(digits_data)\n",
    "head(colnames(digits_data),4)\n",
    "tail(colnames(digits_data),4)\n",
    "\n",
    "head(digits_data[1:2,1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us convert the labels into factor and visualize their distribution. We use only the first 5000 images for training and the rest for testing purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training set - converting labels into factors\n",
    "digits_data$label <- factor(digits_data$label, levels = 0:9)\n",
    "i <- 1:5000\n",
    "digits_x <- digits_data[i,-1]\n",
    "digits_y <- digits_data[i,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our MLP with the caret wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  3985\n",
      "initial  value 12851.237697 \n",
      "iter  10 value 8565.005426\n",
      "iter  20 value 8084.308534\n",
      "iter  30 value 7819.255365\n",
      "iter  40 value 7729.169609\n",
      "iter  50 value 7494.533999\n",
      "iter  60 value 7439.809544\n",
      "iter  70 value 7403.228611\n",
      "iter  80 value 7360.001908\n",
      "iter  90 value 7201.524437\n",
      "iter 100 value 7127.021688\n",
      "final  value 7127.021688 \n",
      "stopped after 100 iterations\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "\n",
    "digits_model <- train(x = digits_x,y = digits_y, method = \"nnet\", \n",
    "                      tuneGrid = expand.grid(.size = c(5), .decay= 0.1 ),\n",
    "                      trControl = trainControl(method = \"none\", seeds = seed),\n",
    "                      MaxNWts = 10000,\n",
    "                      maxit = 100\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's find how accurate is this model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the images\n",
    "digits_pred <- predict(digits_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "           digits_y\n",
       "digits_pred   0   1   2   3   4   5   6   7   8   9\n",
       "          0 445   3 169  77  53 215 347   4  40  10\n",
       "          1   0 495  11   2   2   1   1  16  23   7\n",
       "          2   0  45 117   1   1  32  10   7  18   0\n",
       "          3  32   2 118 231 130  58   7   6  45  50\n",
       "          4   9   0   2   1 120  44   6  11   2  45\n",
       "          5   0   0   0   0   0   0   0   0   0   0\n",
       "          6   0   3  30   0   3  16 142   0   0   1\n",
       "          7   0   3   8   0   2   0   0 384   0   9\n",
       "          8   6   7  86 162  14  96   2  24 345  37\n",
       "          9   2   0   4   6 152   7   1  54   4 319\n",
       "\n",
       "Overall Statistics\n",
       "                                        \n",
       "               Accuracy : 0.52          \n",
       "                 95% CI : (0.506, 0.534)\n",
       "    No Information Rate : 0.112         \n",
       "    P-Value [Acc > NIR] : <2e-16        \n",
       "                                        \n",
       "                  Kappa : 0.466         \n",
       " Mcnemar's Test P-Value : NA            \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5\n",
       "Sensitivity            0.9008    0.887   0.2147   0.4813   0.2516   0.0000\n",
       "Specificity            0.7963    0.986   0.9744   0.9009   0.9735   1.0000\n",
       "Pos Pred Value         0.3265    0.887   0.5065   0.3402   0.5000      NaN\n",
       "Neg Pred Value         0.9865    0.986   0.9103   0.9424   0.9250   0.9062\n",
       "Prevalence             0.0988    0.112   0.1090   0.0960   0.0954   0.0938\n",
       "Detection Rate         0.0890    0.099   0.0234   0.0462   0.0240   0.0000\n",
       "Detection Prevalence   0.2726    0.112   0.0462   0.1358   0.0480   0.0000\n",
       "Balanced Accuracy      0.8485    0.936   0.5945   0.6911   0.6125   0.5000\n",
       "                     Class: 6 Class: 7 Class: 8 Class: 9\n",
       "Sensitivity            0.2752   0.7589   0.7233   0.6674\n",
       "Specificity            0.9882   0.9951   0.9040   0.9491\n",
       "Pos Pred Value         0.7282   0.9458   0.4429   0.5811\n",
       "Neg Pred Value         0.9222   0.9734   0.9687   0.9643\n",
       "Prevalence             0.1032   0.1012   0.0954   0.0956\n",
       "Detection Rate         0.0284   0.0768   0.0690   0.0638\n",
       "Detection Prevalence   0.0390   0.0812   0.1558   0.1098\n",
       "Balanced Accuracy      0.6317   0.8770   0.8137   0.8083"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the confusion matrix\n",
    "caret::confusionMatrix(xtabs(~digits_pred + digits_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nCQIECQIECQIECQIECQIECQIECQL+F+BPl6MEEGnxAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of Predicted labels\n",
    "barplot(table(digits_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to predict new examples that our model we have never seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>8</li>\n",
       "\t<li>7</li>\n",
       "\t<li>3</li>\n",
       "\t<li>0</li>\n",
       "\t<li>9</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 8\n",
       "\\item 7\n",
       "\\item 3\n",
       "\\item 0\n",
       "\\item 9\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 8\n",
       "2. 7\n",
       "3. 3\n",
       "4. 0\n",
       "5. 9\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 8 7 3 0 9\n",
       "Levels: 0 1 2 3 4 5 6 7 8 9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>8</li>\n",
       "\t<li>7</li>\n",
       "\t<li>2</li>\n",
       "\t<li>6</li>\n",
       "\t<li>3</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 8\n",
       "\\item 7\n",
       "\\item 2\n",
       "\\item 6\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 8\n",
       "2. 7\n",
       "3. 2\n",
       "4. 6\n",
       "5. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 8 7 2 6 3\n",
       "Levels: 0 1 2 3 4 5 6 7 8 9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# recovering 5 images we have't used during training\n",
    "i <- 5001:5005\n",
    "new_digits_x <- digits_data[i,-1]\n",
    "new_digits_y <- digits_data[i, 1]\n",
    "predict(digits_model, newdata = new_digits_x)\n",
    "new_digits_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the above result that the predicted value for 8 & 7 digit is correct but for 2,6 & 3 is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can show the probabilities per class just adding the type parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>0</th><th scope=col>1</th><th scope=col>2</th><th scope=col>3</th><th scope=col>4</th><th scope=col>5</th><th scope=col>6</th><th scope=col>7</th><th scope=col>8</th><th scope=col>9</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>5001</th><td>2.84e-04</td><td>1.46e-06</td><td>1.11e-01</td><td>3.75e-01</td><td>0.000602</td><td>9.57e-02</td><td>1.03e-06</td><td>0.001975</td><td>4.14e-01</td><td>0.001640</td></tr>\n",
       "\t<tr><th scope=row>5002</th><td>2.54e-07</td><td>2.32e-13</td><td>6.63e-08</td><td>1.33e-07</td><td>0.001163</td><td>1.03e-06</td><td>2.18e-07</td><td>0.998101</td><td>1.76e-08</td><td>0.000734</td></tr>\n",
       "\t<tr><th scope=row>5003</th><td>2.65e-02</td><td>1.49e-07</td><td>1.77e-01</td><td>2.90e-01</td><td>0.199263</td><td>1.39e-01</td><td>3.64e-03</td><td>0.013100</td><td>4.96e-02</td><td>0.101339</td></tr>\n",
       "\t<tr><th scope=row>5004</th><td>3.50e-01</td><td>1.28e-08</td><td>1.43e-01</td><td>5.80e-02</td><td>0.039692</td><td>1.55e-01</td><td>2.34e-01</td><td>0.000593</td><td>1.68e-02</td><td>0.002870</td></tr>\n",
       "\t<tr><th scope=row>5005</th><td>4.18e-05</td><td>1.13e-07</td><td>1.97e-03</td><td>8.22e-03</td><td>0.227825</td><td>8.80e-03</td><td>2.19e-05</td><td>0.115423</td><td>4.06e-03</td><td>0.633641</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       "  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\\\\n",
       "\\hline\n",
       "\t5001 & 2.84e-04 & 1.46e-06 & 1.11e-01 & 3.75e-01 & 0.000602 & 9.57e-02 & 1.03e-06 & 0.001975 & 4.14e-01 & 0.001640\\\\\n",
       "\t5002 & 2.54e-07 & 2.32e-13 & 6.63e-08 & 1.33e-07 & 0.001163 & 1.03e-06 & 2.18e-07 & 0.998101 & 1.76e-08 & 0.000734\\\\\n",
       "\t5003 & 2.65e-02 & 1.49e-07 & 1.77e-01 & 2.90e-01 & 0.199263 & 1.39e-01 & 3.64e-03 & 0.013100 & 4.96e-02 & 0.101339\\\\\n",
       "\t5004 & 3.50e-01 & 1.28e-08 & 1.43e-01 & 5.80e-02 & 0.039692 & 1.55e-01 & 2.34e-01 & 0.000593 & 1.68e-02 & 0.002870\\\\\n",
       "\t5005 & 4.18e-05 & 1.13e-07 & 1.97e-03 & 8.22e-03 & 0.227825 & 8.80e-03 & 2.19e-05 & 0.115423 & 4.06e-03 & 0.633641\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | \n",
       "|---|---|---|---|---|\n",
       "| 5001 | 2.84e-04 | 1.46e-06 | 1.11e-01 | 3.75e-01 | 0.000602 | 9.57e-02 | 1.03e-06 | 0.001975 | 4.14e-01 | 0.001640 | \n",
       "| 5002 | 2.54e-07 | 2.32e-13 | 6.63e-08 | 1.33e-07 | 0.001163 | 1.03e-06 | 2.18e-07 | 0.998101 | 1.76e-08 | 0.000734 | \n",
       "| 5003 | 2.65e-02 | 1.49e-07 | 1.77e-01 | 2.90e-01 | 0.199263 | 1.39e-01 | 3.64e-03 | 0.013100 | 4.96e-02 | 0.101339 | \n",
       "| 5004 | 3.50e-01 | 1.28e-08 | 1.43e-01 | 5.80e-02 | 0.039692 | 1.55e-01 | 2.34e-01 | 0.000593 | 1.68e-02 | 0.002870 | \n",
       "| 5005 | 4.18e-05 | 1.13e-07 | 1.97e-03 | 8.22e-03 | 0.227825 | 8.80e-03 | 2.19e-05 | 0.115423 | 4.06e-03 | 0.633641 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     0        1        2        3        4        5        6        7       \n",
       "5001 2.84e-04 1.46e-06 1.11e-01 3.75e-01 0.000602 9.57e-02 1.03e-06 0.001975\n",
       "5002 2.54e-07 2.32e-13 6.63e-08 1.33e-07 0.001163 1.03e-06 2.18e-07 0.998101\n",
       "5003 2.65e-02 1.49e-07 1.77e-01 2.90e-01 0.199263 1.39e-01 3.64e-03 0.013100\n",
       "5004 3.50e-01 1.28e-08 1.43e-01 5.80e-02 0.039692 1.55e-01 2.34e-01 0.000593\n",
       "5005 4.18e-05 1.13e-07 1.97e-03 8.22e-03 0.227825 8.80e-03 2.19e-05 0.115423\n",
       "     8        9       \n",
       "5001 4.14e-01 0.001640\n",
       "5002 1.76e-08 0.000734\n",
       "5003 4.96e-02 0.101339\n",
       "5004 1.68e-02 0.002870\n",
       "5005 4.06e-03 0.633641"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(digits_model, newdata = new_digits_x, type = 'prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's visualize the images to understand where the model is failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAABIFBMVEUAAAAEBAQGBgYICAgN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\nAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing the image\n",
    "\n",
    "vec <- new_digits_x[3, ]\n",
    "vec <- data.matrix(vec)\n",
    "\n",
    "img <- matrix(vec, ncol=28)\n",
    "image(img[,28:1], col=grey(seq(0,1,length = 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "+ Discussed basic concepts of DL, the fundamental computational unit the Perceptron \n",
    "+ Explored Two-layers Neural Networks (ML P) and available ANNs packages in R \n",
    "+ Solved a real-world classification problem with MLPs: \n",
    "    + Handwritten digit recognition in R \n",
    "+ Obtained probabilistic predictions with ANNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-hidden-layer ANN\n",
    "\n",
    "In the previous section we have learned about the MLP :\n",
    "\n",
    "+ In principle a universal approximator \n",
    "+ In practice, it needs an exponential number of hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/mlp.png' alt='Multi Layer Perceptron' align='left' height='50%' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Multi-hidden-layer architecture follows the same idea and structure of an MLP but using more layers to: \n",
    "\n",
    "+ Build a hierarchical feature representation of the data \n",
    "+ Solve more complex problems in an efficient way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/mhlp.png' alt='Multi Hidden Layer Perceptron' align='left' height='50%' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Choose the Best Architecture for Your Problem\n",
    "\n",
    "+ Choosing the best architecture has not a grounded and formal theory behind it\n",
    "+ It is essentially chosen by a trial-error process \n",
    "+ The common tricks used to speed-up the process are: \n",
    "     + Starting from a known architecture used for a similar problem \n",
    "     + Starting from the first hidden-layer and adding more and more layers until the accuracy saturates \n",
    " \n",
    "### Limitations of the Multi-hidden-layer ANNs \n",
    "\n",
    "+ Artificial Neural Networks with more than one hidden layer are powerful tools to solve almost any kind of prediction problem, however, they have some severe limitations: \n",
    "    + If we want to scale-up our model using more neurons ( A 227x227 image means more than 50,000 input units!) the problem becomes intractable.\n",
    "    + They are not robust with regards to simple data transformation in the input patterns (Like translations and rotations when dealing with images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Hyper-Parameters — Why Is It Important? \n",
    "\n",
    "+ In machine learning, the term hyper-parameters is used to distinguish them from standard model parameters, that we want to learn through optimization process: \n",
    "    + They define higher level concepts about the model such as complexity, or capacity to learn \n",
    "    + They can't be learned directly from the data in the standard model training process and need to be predefined \n",
    "    + They are critical for the good performance of a model, so be careful \n",
    "\n",
    "### The Main Hyper-Parameters to Tune \n",
    "\n",
    "+ For ANNs we have many hyper-parameters to tune and this is often pointed out as the major downside of these models: \n",
    "    + Number of hidden layers \n",
    "    + Number of hidden units \n",
    "    + Number of training iterations \n",
    "    + Learning rate \n",
    "    + Regularization \n",
    "\n",
    "### The problem of overfitting \n",
    "\n",
    "+ The problem of overfitting regards the complexity of the model: \n",
    "    + Adding more parameters to the network won't always leads to improvements on the accuracy of out-of-sample patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/overfitting.png' alt='overfitting' align='left' height='50%' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ll/L2 and Dropout Regularization \n",
    "\n",
    "+ All the regularization methods applied to ANNs have a common ground: \n",
    "    + Minimize the influence of each parameter or their total number inducing the sparsity of the network \n",
    "+ The most common methods are:\n",
    "    + L1\n",
    "    + L2\n",
    "    + Dropout\n",
    "\n",
    "<img src='./resources/l1.png' alt='l1' align='left' height='50%' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/l2.png' alt='l2' align='left' height='50%' width='50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation, and Test Splits \n",
    "\n",
    "+ Apart from only evaluating the performance of our models on the training data, in order to understand how our model behaves after the real deployment we should.\n",
    "    + Always split the dataset in three parts: Train, Validation and Test sets \n",
    "    + Around (40%, 30%, 3096) depending on the dataset size (eventually with a random sampling) \n",
    "+ Why do we need a validations set \n",
    "    + Because if we tune the hyper-parameters based on the results on the test set we end up overfitting it as well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The H20 package \n",
    "\n",
    "+ The H20 package is very powerful package. \n",
    "    + Many Machine Learning algorithms available \n",
    "    + Well suited for distributed computing \n",
    "    + Much more flexible then nnet and caret regarding ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/h2o.png' alt='H2O' align='left' height='30%' width='30%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use H2O directly from R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies and H2O package\n",
    "#install.packages(c(\"statmod\", \"RCurl\", \"jsonlite\", \"h2o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'h2o' was built under R version 3.4.3\"\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Your next step is to start H2O:\n",
      "    > h2o.init()\n",
      "\n",
      "For H2O package documentation, ask for help:\n",
      "    > ??h2o\n",
      "\n",
      "After starting H2O, you can use the Web UI at http://localhost:54321\n",
      "For more information visit http://docs.h2o.ai\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Attaching package: 'h2o'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cor, sd, var\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n",
      "    colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n",
      "    log10, log1p, log2, round, signif, trunc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's load H2O and start up an H2O cluster\n",
    "library(h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "H2O is not running yet, starting it now...\n",
      "\n",
      "Note:  In case of errors look at the following log files:\n",
      "    C:\\Users\\Public\\Documents\\iSkysoft\\CreatorTemp\\RtmpcHqoD0/h2o_Sarfaraz_started_from_r.out\n",
      "    C:\\Users\\Public\\Documents\\iSkysoft\\CreatorTemp\\RtmpcHqoD0/h2o_Sarfaraz_started_from_r.err\n",
      "\n",
      "\n",
      "Starting H2O JVM and connecting: .... Connection successful!\n",
      "\n",
      "R is connected to the H2O cluster: \n",
      "    H2O cluster uptime:         13 seconds 524 milliseconds \n",
      "    H2O cluster version:        3.16.0.2 \n",
      "    H2O cluster version age:    2 months and 3 days  \n",
      "    H2O cluster name:           H2O_started_from_R_Sarfaraz_qiy213 \n",
      "    H2O cluster total nodes:    1 \n",
      "    H2O cluster total memory:   0.86 GB \n",
      "    H2O cluster total cores:    4 \n",
      "    H2O cluster allowed cores:  4 \n",
      "    H2O cluster healthy:        TRUE \n",
      "    H2O Connection ip:          localhost \n",
      "    H2O Connection port:        54321 \n",
      "    H2O Connection proxy:       NA \n",
      "    H2O Internal Security:      FALSE \n",
      "    H2O API Extensions:         Algos, AutoML, Core V3, Core V4 \n",
      "    R Version:                  R version 3.4.2 (2017-09-28) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initiate H2O package\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading MNIST Data\n",
    "\n",
    "digits_data <- read.csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Let's now split the datasets in TRAIN, VALIDATION and TEST data sets.\n",
    "+ For a more principled randomized data partitioning see [Create Data Partition](https://www.rdocumentation.org/packages/caret/versions/6.0-73/topics/createDataPartition) or [Sample](https://stat.ethz.ch/R-manual/R-devel/library/base/html/sample.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into train, validation and test data sets\n",
    "\n",
    "train <- digits_data[1:5000, ]\n",
    "valid <- digits_data[5001:10000, ]\n",
    "test <- digits_data[10001:15000, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "# Convert data into h2o format\n",
    "\n",
    "train$label <- as.factor(train$label)\n",
    "train_h2o <- as.h2o(train)\n",
    "test_h2o <- as.h2o(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model with two hidden layer of ten units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in .h2o.startModelJob(algo, params, h2oRestApiVersion):\n",
      "\"Dropping bad and constant columns: [pixel729, pixel448, pixel449, pixel724, pixel725, pixel726, pixel727, pixel728, pixel560, pixel52, pixel51, pixel54, pixel53, pixel168, pixel56, pixel169, pixel55, pixel58, pixel57, pixel59, pixel280, pixel559, pixel671, pixel672, pixel673, pixel674, pixel392, pixel393, pixel700, pixel701, pixel308, pixel141, pixel142, pixel780, pixel781, pixel782, pixel420, pixel783, pixel421, pixel140, pixel139, pixel777, pixel778, pixel779, pixel8, pixel9, pixel6, pixel7, pixel4, pixel5, pixel60, pixel252, pixel2, pixel3, pixel0, pixel1, pixel532, pixel644, pixel645, pixel364, pixel760, pixel10, pixel365, pixel12, pixel11, pixel643, pixel14, pixel13, pixel16, pixel15, pixel18, pixel17, pixel19, pixel754, pixel755, pixel756, pixel757, pixel758, pixel759, pixel83, pixel196, pixel82, pixel197, pixel85, pixel110, pixel84, pixel111, pixel87, pixel112, pixel86, pixel113, pixel476, pixel114, pixel477, pixel752, pixel88, pixel753, pixel504, pixel30, pixel32, pixel31, pixel223, pixel587, pixel33, pixel336, pixel699, pixel732, pixel615, pixel21, pixel20, pixel23, pixel697, pixel730, pixel22, pixel335, pixel698, pixel731, pixel25, pixel24, pixel27, pixel26, pixel29, pixel28].\n",
      "\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |=====================                                                 |  30%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "# training a deep learning model:\n",
    "\n",
    "dl_model <- h2o.deeplearning( x = 2:785, y = 1 , \n",
    "                 training_frame = train_h2o,\n",
    "                hidden = c(5), # 1 hidden layer of 5 units\n",
    "                seed = 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can predict the class labels of the test set and compute the out-of-sample accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>0</th><th scope=col>1</th><th scope=col>2</th><th scope=col>3</th><th scope=col>4</th><th scope=col>5</th><th scope=col>6</th><th scope=col>7</th><th scope=col>8</th><th scope=col>9</th><th scope=col>Error</th><th scope=col>Rate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>401          </td><td>  0          </td><td>  0          </td><td>  3          </td><td>  6          </td><td> 12          </td><td> 8           </td><td>  1          </td><td>  20         </td><td>  7          </td><td>0.1245       </td><td>57 / 458     </td></tr>\n",
       "\t<tr><th scope=row>1</th><td>  0          </td><td>550          </td><td>  4          </td><td>  7          </td><td>  0          </td><td>  4          </td><td> 1           </td><td> 13          </td><td>  18         </td><td> 10          </td><td>0.0939       </td><td>57 / 607     </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>  6          </td><td>  8          </td><td>180          </td><td> 48          </td><td> 97          </td><td>  3          </td><td> 5           </td><td> 23          </td><td>  76         </td><td> 23          </td><td>0.6162       </td><td>289 / 469    </td></tr>\n",
       "\t<tr><th scope=row>3</th><td> 15          </td><td> 17          </td><td> 12          </td><td>360          </td><td> 10          </td><td> 31          </td><td> 3           </td><td> 12          </td><td>  14         </td><td> 44          </td><td>0.3050       </td><td>158 / 518    </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>  0          </td><td>  4          </td><td> 71          </td><td>  2          </td><td>265          </td><td>  1          </td><td> 5           </td><td> 16          </td><td> 108         </td><td> 37          </td><td>0.4794       </td><td>244 / 509    </td></tr>\n",
       "\t<tr><th scope=row>5</th><td> 22          </td><td>  0          </td><td>  4          </td><td> 13          </td><td>  7          </td><td>269          </td><td> 4           </td><td> 18          </td><td>  67         </td><td> 48          </td><td>0.4049       </td><td>183 / 452    </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>  8          </td><td>  3          </td><td>  2          </td><td>  6          </td><td> 11          </td><td>  2          </td><td>10           </td><td> 11          </td><td> 427         </td><td>  7          </td><td>0.9795       </td><td>477 / 487    </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>  4          </td><td> 23          </td><td> 20          </td><td>  5          </td><td>  4          </td><td> 28          </td><td> 2           </td><td>405          </td><td>   6         </td><td> 14          </td><td>0.2074       </td><td>106 / 511    </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>  2          </td><td> 28          </td><td>  5          </td><td> 15          </td><td>  9          </td><td> 18          </td><td> 9           </td><td> 14          </td><td> 360         </td><td> 38          </td><td>0.2771       </td><td>138 / 498    </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>  4          </td><td> 16          </td><td> 43          </td><td>  6          </td><td> 12          </td><td> 17          </td><td> 3           </td><td> 23          </td><td>  49         </td><td>318          </td><td>0.3523       </td><td>173 / 491    </td></tr>\n",
       "\t<tr><th scope=row>Totals</th><td>462          </td><td>649          </td><td>341          </td><td>465          </td><td>421          </td><td>385          </td><td>50           </td><td>536          </td><td>1145         </td><td>546          </td><td>0.3764       </td><td>1,882 / 5,000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       "  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & Error & Rate\\\\\n",
       "\\hline\n",
       "\t0 & 401           &   0           &   0           &   3           &   6           &  12           &  8            &   1           &   20          &   7           & 0.1245        & 57 / 458     \\\\\n",
       "\t1 &   0           & 550           &   4           &   7           &   0           &   4           &  1            &  13           &   18          &  10           & 0.0939        & 57 / 607     \\\\\n",
       "\t2 &   6           &   8           & 180           &  48           &  97           &   3           &  5            &  23           &   76          &  23           & 0.6162        & 289 / 469    \\\\\n",
       "\t3 &  15           &  17           &  12           & 360           &  10           &  31           &  3            &  12           &   14          &  44           & 0.3050        & 158 / 518    \\\\\n",
       "\t4 &   0           &   4           &  71           &   2           & 265           &   1           &  5            &  16           &  108          &  37           & 0.4794        & 244 / 509    \\\\\n",
       "\t5 &  22           &   0           &   4           &  13           &   7           & 269           &  4            &  18           &   67          &  48           & 0.4049        & 183 / 452    \\\\\n",
       "\t6 &   8           &   3           &   2           &   6           &  11           &   2           & 10            &  11           &  427          &   7           & 0.9795        & 477 / 487    \\\\\n",
       "\t7 &   4           &  23           &  20           &   5           &   4           &  28           &  2            & 405           &    6          &  14           & 0.2074        & 106 / 511    \\\\\n",
       "\t8 &   2           &  28           &   5           &  15           &   9           &  18           &  9            &  14           &  360          &  38           & 0.2771        & 138 / 498    \\\\\n",
       "\t9 &   4           &  16           &  43           &   6           &  12           &  17           &  3            &  23           &   49          & 318           & 0.3523        & 173 / 491    \\\\\n",
       "\tTotals & 462           & 649           & 341           & 465           & 421           & 385           & 50            & 536           & 1145          & 546           & 0.3764        & 1,882 / 5,000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | Error | Rate | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0 | 401           |   0           |   0           |   3           |   6           |  12           |  8            |   1           |   20          |   7           | 0.1245        | 57 / 458      | \n",
       "| 1 |   0           | 550           |   4           |   7           |   0           |   4           |  1            |  13           |   18          |  10           | 0.0939        | 57 / 607      | \n",
       "| 2 |   6           |   8           | 180           |  48           |  97           |   3           |  5            |  23           |   76          |  23           | 0.6162        | 289 / 469     | \n",
       "| 3 |  15           |  17           |  12           | 360           |  10           |  31           |  3            |  12           |   14          |  44           | 0.3050        | 158 / 518     | \n",
       "| 4 |   0           |   4           |  71           |   2           | 265           |   1           |  5            |  16           |  108          |  37           | 0.4794        | 244 / 509     | \n",
       "| 5 |  22           |   0           |   4           |  13           |   7           | 269           |  4            |  18           |   67          |  48           | 0.4049        | 183 / 452     | \n",
       "| 6 |   8           |   3           |   2           |   6           |  11           |   2           | 10            |  11           |  427          |   7           | 0.9795        | 477 / 487     | \n",
       "| 7 |   4           |  23           |  20           |   5           |   4           |  28           |  2            | 405           |    6          |  14           | 0.2074        | 106 / 511     | \n",
       "| 8 |   2           |  28           |   5           |  15           |   9           |  18           |  9            |  14           |  360          |  38           | 0.2771        | 138 / 498     | \n",
       "| 9 |   4           |  16           |  43           |   6           |  12           |  17           |  3            |  23           |   49          | 318           | 0.3523        | 173 / 491     | \n",
       "| Totals | 462           | 649           | 341           | 465           | 421           | 385           | 50            | 536           | 1145          | 546           | 0.3764        | 1,882 / 5,000 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       0   1   2   3   4   5   6  7   8    9   Error  Rate         \n",
       "0      401   0   0   3   6  12  8   1   20   7 0.1245 57 / 458     \n",
       "1        0 550   4   7   0   4  1  13   18  10 0.0939 57 / 607     \n",
       "2        6   8 180  48  97   3  5  23   76  23 0.6162 289 / 469    \n",
       "3       15  17  12 360  10  31  3  12   14  44 0.3050 158 / 518    \n",
       "4        0   4  71   2 265   1  5  16  108  37 0.4794 244 / 509    \n",
       "5       22   0   4  13   7 269  4  18   67  48 0.4049 183 / 452    \n",
       "6        8   3   2   6  11   2 10  11  427   7 0.9795 477 / 487    \n",
       "7        4  23  20   5   4  28  2 405    6  14 0.2074 106 / 511    \n",
       "8        2  28   5  15   9  18  9  14  360  38 0.2771 138 / 498    \n",
       "9        4  16  43   6  12  17  3  23   49 318 0.3523 173 / 491    \n",
       "Totals 462 649 341 465 421 385 50 536 1145 546 0.3764 1,882 / 5,000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting the images\n",
    "digits_pred <- predict(dl_model, test_h2o)\n",
    "h2o.confusionMatrix(dl_model, test_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in .h2o.startModelJob(algo, params, h2oRestApiVersion):\n",
      "\"Dropping bad and constant columns: [pixel729, pixel448, pixel449, pixel724, pixel725, pixel726, pixel727, pixel728, pixel560, pixel52, pixel51, pixel54, pixel53, pixel168, pixel56, pixel169, pixel55, pixel58, pixel57, pixel59, pixel280, pixel559, pixel671, pixel672, pixel673, pixel674, pixel392, pixel393, pixel700, pixel701, pixel308, pixel141, pixel142, pixel780, pixel781, pixel782, pixel420, pixel783, pixel421, pixel140, pixel139, pixel777, pixel778, pixel779, pixel8, pixel9, pixel6, pixel7, pixel4, pixel5, pixel60, pixel252, pixel2, pixel3, pixel0, pixel1, pixel532, pixel644, pixel645, pixel364, pixel760, pixel10, pixel365, pixel12, pixel11, pixel643, pixel14, pixel13, pixel16, pixel15, pixel18, pixel17, pixel19, pixel754, pixel755, pixel756, pixel757, pixel758, pixel759, pixel83, pixel196, pixel82, pixel197, pixel85, pixel110, pixel84, pixel111, pixel87, pixel112, pixel86, pixel113, pixel476, pixel114, pixel477, pixel752, pixel88, pixel753, pixel504, pixel30, pixel32, pixel31, pixel223, pixel587, pixel33, pixel336, pixel699, pixel732, pixel615, pixel21, pixel20, pixel23, pixel697, pixel730, pixel22, pixel335, pixel698, pixel731, pixel25, pixel24, pixel27, pixel26, pixel29, pixel28].\n",
      "\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |=======                                                               |  10%\r",
      "  |                                                                            \r",
      "  |===================================                                   |  50%\r",
      "  |                                                                            \r",
      "  |===============================================================       |  90%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "# training a deep learning model:\n",
    "\n",
    "dl_model <- h2o.deeplearning(x = 2:785, y = 1 , \n",
    "                training_frame = train_h2o,\n",
    "                hidden = c(10,10), # 2 hidden layers of 10 units\n",
    "                seed = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>0</th><th scope=col>1</th><th scope=col>2</th><th scope=col>3</th><th scope=col>4</th><th scope=col>5</th><th scope=col>6</th><th scope=col>7</th><th scope=col>8</th><th scope=col>9</th><th scope=col>Error</th><th scope=col>Rate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>419        </td><td>  0        </td><td>  9        </td><td>  5        </td><td>  4        </td><td>  6        </td><td>  1        </td><td>  1        </td><td> 10        </td><td>  3        </td><td>0.0852     </td><td>39 / 458   </td></tr>\n",
       "\t<tr><th scope=row>1</th><td>  0        </td><td>574        </td><td>  2        </td><td>  9        </td><td>  0        </td><td>  7        </td><td>  2        </td><td>  5        </td><td>  6        </td><td>  2        </td><td>0.0544     </td><td>33 / 607   </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>  4        </td><td>  2        </td><td>383        </td><td>  6        </td><td>  6        </td><td> 10        </td><td> 20        </td><td> 15        </td><td> 20        </td><td>  3        </td><td>0.1834     </td><td>86 / 469   </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>  3        </td><td>  5        </td><td> 33        </td><td>424        </td><td>  0        </td><td> 26        </td><td>  5        </td><td>  6        </td><td> 12        </td><td>  4        </td><td>0.1815     </td><td>94 / 518   </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>  3        </td><td>  5        </td><td>  4        </td><td>  1        </td><td>457        </td><td>  2        </td><td> 14        </td><td>  2        </td><td>  0        </td><td> 21        </td><td>0.1022     </td><td>52 / 509   </td></tr>\n",
       "\t<tr><th scope=row>5</th><td> 10        </td><td>  3        </td><td> 10        </td><td> 25        </td><td>  4        </td><td>365        </td><td>  7        </td><td>  3        </td><td> 17        </td><td>  8        </td><td>0.1925     </td><td>87 / 452   </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>  5        </td><td>  5        </td><td> 11        </td><td>  0        </td><td>  5        </td><td> 14        </td><td>440        </td><td>  0        </td><td>  7        </td><td>  0        </td><td>0.0965     </td><td>47 / 487   </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>  3        </td><td>  8        </td><td> 14        </td><td>  5        </td><td>  5        </td><td>  0        </td><td>  0        </td><td>453        </td><td>  1        </td><td> 22        </td><td>0.1135     </td><td>58 / 511   </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>  8        </td><td> 21        </td><td> 19        </td><td> 13        </td><td>  4        </td><td> 14        </td><td>  3        </td><td>  3        </td><td>409        </td><td>  4        </td><td>0.1787     </td><td>89 / 498   </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>  2        </td><td>  8        </td><td>  2        </td><td>  8        </td><td> 30        </td><td>  4        </td><td>  0        </td><td> 20        </td><td>  6        </td><td>411        </td><td>0.1629     </td><td>80 / 491   </td></tr>\n",
       "\t<tr><th scope=row>Totals</th><td>457        </td><td>631        </td><td>487        </td><td>496        </td><td>515        </td><td>448        </td><td>492        </td><td>508        </td><td>488        </td><td>478        </td><td>0.1330     </td><td>665 / 5,000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       "  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & Error & Rate\\\\\n",
       "\\hline\n",
       "\t0 & 419         &   0         &   9         &   5         &   4         &   6         &   1         &   1         &  10         &   3         & 0.0852      & 39 / 458   \\\\\n",
       "\t1 &   0         & 574         &   2         &   9         &   0         &   7         &   2         &   5         &   6         &   2         & 0.0544      & 33 / 607   \\\\\n",
       "\t2 &   4         &   2         & 383         &   6         &   6         &  10         &  20         &  15         &  20         &   3         & 0.1834      & 86 / 469   \\\\\n",
       "\t3 &   3         &   5         &  33         & 424         &   0         &  26         &   5         &   6         &  12         &   4         & 0.1815      & 94 / 518   \\\\\n",
       "\t4 &   3         &   5         &   4         &   1         & 457         &   2         &  14         &   2         &   0         &  21         & 0.1022      & 52 / 509   \\\\\n",
       "\t5 &  10         &   3         &  10         &  25         &   4         & 365         &   7         &   3         &  17         &   8         & 0.1925      & 87 / 452   \\\\\n",
       "\t6 &   5         &   5         &  11         &   0         &   5         &  14         & 440         &   0         &   7         &   0         & 0.0965      & 47 / 487   \\\\\n",
       "\t7 &   3         &   8         &  14         &   5         &   5         &   0         &   0         & 453         &   1         &  22         & 0.1135      & 58 / 511   \\\\\n",
       "\t8 &   8         &  21         &  19         &  13         &   4         &  14         &   3         &   3         & 409         &   4         & 0.1787      & 89 / 498   \\\\\n",
       "\t9 &   2         &   8         &   2         &   8         &  30         &   4         &   0         &  20         &   6         & 411         & 0.1629      & 80 / 491   \\\\\n",
       "\tTotals & 457         & 631         & 487         & 496         & 515         & 448         & 492         & 508         & 488         & 478         & 0.1330      & 665 / 5,000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | Error | Rate | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0 | 419         |   0         |   9         |   5         |   4         |   6         |   1         |   1         |  10         |   3         | 0.0852      | 39 / 458    | \n",
       "| 1 |   0         | 574         |   2         |   9         |   0         |   7         |   2         |   5         |   6         |   2         | 0.0544      | 33 / 607    | \n",
       "| 2 |   4         |   2         | 383         |   6         |   6         |  10         |  20         |  15         |  20         |   3         | 0.1834      | 86 / 469    | \n",
       "| 3 |   3         |   5         |  33         | 424         |   0         |  26         |   5         |   6         |  12         |   4         | 0.1815      | 94 / 518    | \n",
       "| 4 |   3         |   5         |   4         |   1         | 457         |   2         |  14         |   2         |   0         |  21         | 0.1022      | 52 / 509    | \n",
       "| 5 |  10         |   3         |  10         |  25         |   4         | 365         |   7         |   3         |  17         |   8         | 0.1925      | 87 / 452    | \n",
       "| 6 |   5         |   5         |  11         |   0         |   5         |  14         | 440         |   0         |   7         |   0         | 0.0965      | 47 / 487    | \n",
       "| 7 |   3         |   8         |  14         |   5         |   5         |   0         |   0         | 453         |   1         |  22         | 0.1135      | 58 / 511    | \n",
       "| 8 |   8         |  21         |  19         |  13         |   4         |  14         |   3         |   3         | 409         |   4         | 0.1787      | 89 / 498    | \n",
       "| 9 |   2         |   8         |   2         |   8         |  30         |   4         |   0         |  20         |   6         | 411         | 0.1629      | 80 / 491    | \n",
       "| Totals | 457         | 631         | 487         | 496         | 515         | 448         | 492         | 508         | 488         | 478         | 0.1330      | 665 / 5,000 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       0   1   2   3   4   5   6   7   8   9   Error  Rate       \n",
       "0      419   0   9   5   4   6   1   1  10   3 0.0852 39 / 458   \n",
       "1        0 574   2   9   0   7   2   5   6   2 0.0544 33 / 607   \n",
       "2        4   2 383   6   6  10  20  15  20   3 0.1834 86 / 469   \n",
       "3        3   5  33 424   0  26   5   6  12   4 0.1815 94 / 518   \n",
       "4        3   5   4   1 457   2  14   2   0  21 0.1022 52 / 509   \n",
       "5       10   3  10  25   4 365   7   3  17   8 0.1925 87 / 452   \n",
       "6        5   5  11   0   5  14 440   0   7   0 0.0965 47 / 487   \n",
       "7        3   8  14   5   5   0   0 453   1  22 0.1135 58 / 511   \n",
       "8        8  21  19  13   4  14   3   3 409   4 0.1787 89 / 498   \n",
       "9        2   8   2   8  30   4   0  20   6 411 0.1629 80 / 491   \n",
       "Totals 457 631 487 496 515 448 492 508 488 478 0.1330 665 / 5,000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting the images\n",
    "digits_pred <- predict(dl_model, test_h2o)\n",
    "h2o.confusionMatrix(dl_model, test_h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Breast-Cancer Dataset \n",
    "\n",
    "+ The Breast-Cancer Dataset is a collection of Dr. Wolberg real clinical cases \n",
    "+ Based only on different biomedical attributes encoded as integer values \n",
    "+ Key features: \n",
    "    + 2 classes \n",
    "    + 699 records \n",
    "    + 10 attributes \n",
    "\n",
    "Here, we'll how to use the H2O package in order to detect cancer given a number of biomedical attributes. The data we are going to use is the [Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer)\n",
    "which can be loaded directly through the **mlbench** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing mlbench package\n",
    "#install.packages('mlbench', repos='https://cran.r-project.org/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading packages \n",
    "\n",
    "library(mlbench)\n",
    "library(h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Connection successful!\n",
      "\n",
      "R is connected to the H2O cluster: \n",
      "    H2O cluster uptime:         1 minutes 12 seconds \n",
      "    H2O cluster version:        3.16.0.2 \n",
      "    H2O cluster version age:    2 months and 3 days  \n",
      "    H2O cluster name:           H2O_started_from_R_Sarfaraz_qiy213 \n",
      "    H2O cluster total nodes:    1 \n",
      "    H2O cluster total memory:   0.78 GB \n",
      "    H2O cluster total cores:    4 \n",
      "    H2O cluster allowed cores:  4 \n",
      "    H2O cluster healthy:        TRUE \n",
      "    H2O Connection ip:          localhost \n",
      "    H2O Connection port:        54321 \n",
      "    H2O Connection proxy:       NA \n",
      "    H2O Internal Security:      FALSE \n",
      "    H2O API Extensions:         Algos, AutoML, Core V3, Core V4 \n",
      "    R Version:                  R version 3.4.2 (2017-09-28) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initializing h2o\n",
    "\n",
    "h2o.init(nthread = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Id</th><th scope=col>Cl.thickness</th><th scope=col>Cell.size</th><th scope=col>Cell.shape</th><th scope=col>Marg.adhesion</th><th scope=col>Epith.c.size</th><th scope=col>Bare.nuclei</th><th scope=col>Bl.cromatin</th><th scope=col>Normal.nucleoli</th><th scope=col>Mitoses</th><th scope=col>Class</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1000025  </td><td>5        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1002945  </td><td>5        </td><td>4        </td><td>4        </td><td>5        </td><td>7        </td><td>10       </td><td>3        </td><td>2        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1015425  </td><td>3        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>2        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1016277  </td><td>6        </td><td>8        </td><td>8        </td><td>1        </td><td>3        </td><td>4        </td><td>3        </td><td>7        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1017023  </td><td>4        </td><td>1        </td><td>1        </td><td>3        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1017122  </td><td>8        </td><td>10       </td><td>10       </td><td>8        </td><td>7        </td><td>10       </td><td>9        </td><td>7        </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1018099  </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>10       </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1018561  </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1033078  </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>5        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1033078  </td><td>4        </td><td>2        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1035283  </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1036172  </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1041801  </td><td>5        </td><td>3        </td><td>3        </td><td>3        </td><td>2        </td><td>3        </td><td>4        </td><td>4        </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1043999  </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>3        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1044572  </td><td>8        </td><td>7        </td><td>5        </td><td>10       </td><td>7        </td><td>9        </td><td>5        </td><td>5        </td><td>4        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1047630  </td><td>7        </td><td>4        </td><td>6        </td><td>4        </td><td>6        </td><td>1        </td><td>4        </td><td>3        </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1048672  </td><td>4        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1049815  </td><td>4        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1050670  </td><td>10       </td><td>7        </td><td>7        </td><td>6        </td><td>4        </td><td>10       </td><td>4        </td><td>1        </td><td>2        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1050718  </td><td>6        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1054590  </td><td>7        </td><td>3        </td><td>2        </td><td>10       </td><td>5        </td><td>10       </td><td>5        </td><td>4        </td><td>4        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1054593  </td><td>10       </td><td>5        </td><td>5        </td><td>3        </td><td>6        </td><td>7        </td><td>7        </td><td>10       </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1056784  </td><td>3        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1057013  </td><td>8        </td><td>4        </td><td>5        </td><td>1        </td><td>2        </td><td>NA       </td><td>7        </td><td>3        </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1059552  </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1065726  </td><td>5        </td><td>2        </td><td>3        </td><td>4        </td><td>2        </td><td>7        </td><td>3        </td><td>6        </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1066373  </td><td>3        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1066979  </td><td>5        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1067444  </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1070935  </td><td>1        </td><td>1        </td><td>3        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>1350423  </td><td>5        </td><td>10       </td><td>10       </td><td>8        </td><td>5        </td><td>5        </td><td>7        </td><td>10       </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1352848  </td><td>3        </td><td>10       </td><td>7        </td><td>8        </td><td>5        </td><td>8        </td><td>7        </td><td>4        </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1353092  </td><td>3        </td><td>2        </td><td>1        </td><td>2        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1354840  </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1354840  </td><td>5        </td><td>3        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1355260  </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1365075  </td><td>4        </td><td>1        </td><td>4        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1365328  </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1368267  </td><td>5        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1368273  </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1368882  </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>1369821  </td><td>10       </td><td>10       </td><td>10       </td><td>10       </td><td>5        </td><td>10       </td><td>10       </td><td>10       </td><td>7        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1371026  </td><td>5        </td><td>10       </td><td>10       </td><td>10       </td><td>4        </td><td>10       </td><td>5        </td><td>6        </td><td>3        </td><td>malignant</td></tr>\n",
       "\t<tr><td>1371920  </td><td>5        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>3        </td><td>2        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>466906   </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>466906   </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>534555   </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>536708   </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>566346   </td><td>3        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>3        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>603148   </td><td>4        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>654546   </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>8        </td><td>benign   </td></tr>\n",
       "\t<tr><td>654546   </td><td>1        </td><td>1        </td><td>1        </td><td>3        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>695091   </td><td>5        </td><td>10       </td><td>10       </td><td>5        </td><td>4        </td><td>5        </td><td>4        </td><td>4        </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>714039   </td><td>3        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>763235   </td><td>3        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>1        </td><td>2        </td><td>benign   </td></tr>\n",
       "\t<tr><td>776715   </td><td>3        </td><td>1        </td><td>1        </td><td>1        </td><td>3        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>841769   </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>1        </td><td>1        </td><td>1        </td><td>benign   </td></tr>\n",
       "\t<tr><td>888820   </td><td>5        </td><td>10       </td><td>10       </td><td>3        </td><td>7        </td><td>3        </td><td>8        </td><td>10       </td><td>2        </td><td>malignant</td></tr>\n",
       "\t<tr><td>897471   </td><td>4        </td><td>8        </td><td>6        </td><td>4        </td><td>3        </td><td>4        </td><td>10       </td><td>6        </td><td>1        </td><td>malignant</td></tr>\n",
       "\t<tr><td>897471   </td><td>4        </td><td>8        </td><td>8        </td><td>5        </td><td>4        </td><td>5        </td><td>10       </td><td>4        </td><td>1        </td><td>malignant</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       " Id & Cl.thickness & Cell.size & Cell.shape & Marg.adhesion & Epith.c.size & Bare.nuclei & Bl.cromatin & Normal.nucleoli & Mitoses & Class\\\\\n",
       "\\hline\n",
       "\t 1000025   & 5         & 1         & 1         & 1         & 2         & 1         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1002945   & 5         & 4         & 4         & 5         & 7         & 10        & 3         & 2         & 1         & benign   \\\\\n",
       "\t 1015425   & 3         & 1         & 1         & 1         & 2         & 2         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1016277   & 6         & 8         & 8         & 1         & 3         & 4         & 3         & 7         & 1         & benign   \\\\\n",
       "\t 1017023   & 4         & 1         & 1         & 3         & 2         & 1         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1017122   & 8         & 10        & 10        & 8         & 7         & 10        & 9         & 7         & 1         & malignant\\\\\n",
       "\t 1018099   & 1         & 1         & 1         & 1         & 2         & 10        & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1018561   & 2         & 1         & 2         & 1         & 2         & 1         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1033078   & 2         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 5         & benign   \\\\\n",
       "\t 1033078   & 4         & 2         & 1         & 1         & 2         & 1         & 2         & 1         & 1         & benign   \\\\\n",
       "\t 1035283   & 1         & 1         & 1         & 1         & 1         & 1         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1036172   & 2         & 1         & 1         & 1         & 2         & 1         & 2         & 1         & 1         & benign   \\\\\n",
       "\t 1041801   & 5         & 3         & 3         & 3         & 2         & 3         & 4         & 4         & 1         & malignant\\\\\n",
       "\t 1043999   & 1         & 1         & 1         & 1         & 2         & 3         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1044572   & 8         & 7         & 5         & 10        & 7         & 9         & 5         & 5         & 4         & malignant\\\\\n",
       "\t 1047630   & 7         & 4         & 6         & 4         & 6         & 1         & 4         & 3         & 1         & malignant\\\\\n",
       "\t 1048672   & 4         & 1         & 1         & 1         & 2         & 1         & 2         & 1         & 1         & benign   \\\\\n",
       "\t 1049815   & 4         & 1         & 1         & 1         & 2         & 1         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1050670   & 10        & 7         & 7         & 6         & 4         & 10        & 4         & 1         & 2         & malignant\\\\\n",
       "\t 1050718   & 6         & 1         & 1         & 1         & 2         & 1         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1054590   & 7         & 3         & 2         & 10        & 5         & 10        & 5         & 4         & 4         & malignant\\\\\n",
       "\t 1054593   & 10        & 5         & 5         & 3         & 6         & 7         & 7         & 10        & 1         & malignant\\\\\n",
       "\t 1056784   & 3         & 1         & 1         & 1         & 2         & 1         & 2         & 1         & 1         & benign   \\\\\n",
       "\t 1057013   & 8         & 4         & 5         & 1         & 2         & NA        & 7         & 3         & 1         & malignant\\\\\n",
       "\t 1059552   & 1         & 1         & 1         & 1         & 2         & 1         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1065726   & 5         & 2         & 3         & 4         & 2         & 7         & 3         & 6         & 1         & malignant\\\\\n",
       "\t 1066373   & 3         & 2         & 1         & 1         & 1         & 1         & 2         & 1         & 1         & benign   \\\\\n",
       "\t 1066979   & 5         & 1         & 1         & 1         & 2         & 1         & 2         & 1         & 1         & benign   \\\\\n",
       "\t 1067444   & 2         & 1         & 1         & 1         & 2         & 1         & 2         & 1         & 1         & benign   \\\\\n",
       "\t 1070935   & 1         & 1         & 3         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ...\\\\\n",
       "\t 1350423   & 5         & 10        & 10        & 8         & 5         & 5         & 7         & 10        & 1         & malignant\\\\\n",
       "\t 1352848   & 3         & 10        & 7         & 8         & 5         & 8         & 7         & 4         & 1         & malignant\\\\\n",
       "\t 1353092   & 3         & 2         & 1         & 2         & 2         & 1         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1354840   & 2         & 1         & 1         & 1         & 2         & 1         & 3         & 1         & 1         & benign   \\\\\n",
       "\t 1354840   & 5         & 3         & 2         & 1         & 3         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 1355260   & 1         & 1         & 1         & 1         & 2         & 1         & 2         & 1         & 1         & benign   \\\\\n",
       "\t 1365075   & 4         & 1         & 4         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 1365328   & 1         & 1         & 2         & 1         & 2         & 1         & 2         & 1         & 1         & benign   \\\\\n",
       "\t 1368267   & 5         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 1368273   & 1         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 1368882   & 2         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 1369821   & 10        & 10        & 10        & 10        & 5         & 10        & 10        & 10        & 7         & malignant\\\\\n",
       "\t 1371026   & 5         & 10        & 10        & 10        & 4         & 10        & 5         & 6         & 3         & malignant\\\\\n",
       "\t 1371920   & 5         & 1         & 1         & 1         & 2         & 1         & 3         & 2         & 1         & benign   \\\\\n",
       "\t 466906    & 1         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 466906    & 1         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 534555    & 1         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 536708    & 1         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 566346    & 3         & 1         & 1         & 1         & 2         & 1         & 2         & 3         & 1         & benign   \\\\\n",
       "\t 603148    & 4         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 654546    & 1         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 8         & benign   \\\\\n",
       "\t 654546    & 1         & 1         & 1         & 3         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 695091    & 5         & 10        & 10        & 5         & 4         & 5         & 4         & 4         & 1         & malignant\\\\\n",
       "\t 714039    & 3         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 763235    & 3         & 1         & 1         & 1         & 2         & 1         & 2         & 1         & 2         & benign   \\\\\n",
       "\t 776715    & 3         & 1         & 1         & 1         & 3         & 2         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 841769    & 2         & 1         & 1         & 1         & 2         & 1         & 1         & 1         & 1         & benign   \\\\\n",
       "\t 888820    & 5         & 10        & 10        & 3         & 7         & 3         & 8         & 10        & 2         & malignant\\\\\n",
       "\t 897471    & 4         & 8         & 6         & 4         & 3         & 4         & 10        & 6         & 1         & malignant\\\\\n",
       "\t 897471    & 4         & 8         & 8         & 5         & 4         & 5         & 10        & 4         & 1         & malignant\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Id | Cl.thickness | Cell.size | Cell.shape | Marg.adhesion | Epith.c.size | Bare.nuclei | Bl.cromatin | Normal.nucleoli | Mitoses | Class | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1000025   | 5         | 1         | 1         | 1         | 2         | 1         | 3         | 1         | 1         | benign    | \n",
       "| 1002945   | 5         | 4         | 4         | 5         | 7         | 10        | 3         | 2         | 1         | benign    | \n",
       "| 1015425   | 3         | 1         | 1         | 1         | 2         | 2         | 3         | 1         | 1         | benign    | \n",
       "| 1016277   | 6         | 8         | 8         | 1         | 3         | 4         | 3         | 7         | 1         | benign    | \n",
       "| 1017023   | 4         | 1         | 1         | 3         | 2         | 1         | 3         | 1         | 1         | benign    | \n",
       "| 1017122   | 8         | 10        | 10        | 8         | 7         | 10        | 9         | 7         | 1         | malignant | \n",
       "| 1018099   | 1         | 1         | 1         | 1         | 2         | 10        | 3         | 1         | 1         | benign    | \n",
       "| 1018561   | 2         | 1         | 2         | 1         | 2         | 1         | 3         | 1         | 1         | benign    | \n",
       "| 1033078   | 2         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 5         | benign    | \n",
       "| 1033078   | 4         | 2         | 1         | 1         | 2         | 1         | 2         | 1         | 1         | benign    | \n",
       "| 1035283   | 1         | 1         | 1         | 1         | 1         | 1         | 3         | 1         | 1         | benign    | \n",
       "| 1036172   | 2         | 1         | 1         | 1         | 2         | 1         | 2         | 1         | 1         | benign    | \n",
       "| 1041801   | 5         | 3         | 3         | 3         | 2         | 3         | 4         | 4         | 1         | malignant | \n",
       "| 1043999   | 1         | 1         | 1         | 1         | 2         | 3         | 3         | 1         | 1         | benign    | \n",
       "| 1044572   | 8         | 7         | 5         | 10        | 7         | 9         | 5         | 5         | 4         | malignant | \n",
       "| 1047630   | 7         | 4         | 6         | 4         | 6         | 1         | 4         | 3         | 1         | malignant | \n",
       "| 1048672   | 4         | 1         | 1         | 1         | 2         | 1         | 2         | 1         | 1         | benign    | \n",
       "| 1049815   | 4         | 1         | 1         | 1         | 2         | 1         | 3         | 1         | 1         | benign    | \n",
       "| 1050670   | 10        | 7         | 7         | 6         | 4         | 10        | 4         | 1         | 2         | malignant | \n",
       "| 1050718   | 6         | 1         | 1         | 1         | 2         | 1         | 3         | 1         | 1         | benign    | \n",
       "| 1054590   | 7         | 3         | 2         | 10        | 5         | 10        | 5         | 4         | 4         | malignant | \n",
       "| 1054593   | 10        | 5         | 5         | 3         | 6         | 7         | 7         | 10        | 1         | malignant | \n",
       "| 1056784   | 3         | 1         | 1         | 1         | 2         | 1         | 2         | 1         | 1         | benign    | \n",
       "| 1057013   | 8         | 4         | 5         | 1         | 2         | NA        | 7         | 3         | 1         | malignant | \n",
       "| 1059552   | 1         | 1         | 1         | 1         | 2         | 1         | 3         | 1         | 1         | benign    | \n",
       "| 1065726   | 5         | 2         | 3         | 4         | 2         | 7         | 3         | 6         | 1         | malignant | \n",
       "| 1066373   | 3         | 2         | 1         | 1         | 1         | 1         | 2         | 1         | 1         | benign    | \n",
       "| 1066979   | 5         | 1         | 1         | 1         | 2         | 1         | 2         | 1         | 1         | benign    | \n",
       "| 1067444   | 2         | 1         | 1         | 1         | 2         | 1         | 2         | 1         | 1         | benign    | \n",
       "| 1070935   | 1         | 1         | 3         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | \n",
       "| 1350423   | 5         | 10        | 10        | 8         | 5         | 5         | 7         | 10        | 1         | malignant | \n",
       "| 1352848   | 3         | 10        | 7         | 8         | 5         | 8         | 7         | 4         | 1         | malignant | \n",
       "| 1353092   | 3         | 2         | 1         | 2         | 2         | 1         | 3         | 1         | 1         | benign    | \n",
       "| 1354840   | 2         | 1         | 1         | 1         | 2         | 1         | 3         | 1         | 1         | benign    | \n",
       "| 1354840   | 5         | 3         | 2         | 1         | 3         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 1355260   | 1         | 1         | 1         | 1         | 2         | 1         | 2         | 1         | 1         | benign    | \n",
       "| 1365075   | 4         | 1         | 4         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 1365328   | 1         | 1         | 2         | 1         | 2         | 1         | 2         | 1         | 1         | benign    | \n",
       "| 1368267   | 5         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 1368273   | 1         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 1368882   | 2         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 1369821   | 10        | 10        | 10        | 10        | 5         | 10        | 10        | 10        | 7         | malignant | \n",
       "| 1371026   | 5         | 10        | 10        | 10        | 4         | 10        | 5         | 6         | 3         | malignant | \n",
       "| 1371920   | 5         | 1         | 1         | 1         | 2         | 1         | 3         | 2         | 1         | benign    | \n",
       "| 466906    | 1         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 466906    | 1         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 534555    | 1         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 536708    | 1         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 566346    | 3         | 1         | 1         | 1         | 2         | 1         | 2         | 3         | 1         | benign    | \n",
       "| 603148    | 4         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 654546    | 1         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 8         | benign    | \n",
       "| 654546    | 1         | 1         | 1         | 3         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 695091    | 5         | 10        | 10        | 5         | 4         | 5         | 4         | 4         | 1         | malignant | \n",
       "| 714039    | 3         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 763235    | 3         | 1         | 1         | 1         | 2         | 1         | 2         | 1         | 2         | benign    | \n",
       "| 776715    | 3         | 1         | 1         | 1         | 3         | 2         | 1         | 1         | 1         | benign    | \n",
       "| 841769    | 2         | 1         | 1         | 1         | 2         | 1         | 1         | 1         | 1         | benign    | \n",
       "| 888820    | 5         | 10        | 10        | 3         | 7         | 3         | 8         | 10        | 2         | malignant | \n",
       "| 897471    | 4         | 8         | 6         | 4         | 3         | 4         | 10        | 6         | 1         | malignant | \n",
       "| 897471    | 4         | 8         | 8         | 5         | 4         | 5         | 10        | 4         | 1         | malignant | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    Id      Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size\n",
       "1   1000025 5            1         1          1             2           \n",
       "2   1002945 5            4         4          5             7           \n",
       "3   1015425 3            1         1          1             2           \n",
       "4   1016277 6            8         8          1             3           \n",
       "5   1017023 4            1         1          3             2           \n",
       "6   1017122 8            10        10         8             7           \n",
       "7   1018099 1            1         1          1             2           \n",
       "8   1018561 2            1         2          1             2           \n",
       "9   1033078 2            1         1          1             2           \n",
       "10  1033078 4            2         1          1             2           \n",
       "11  1035283 1            1         1          1             1           \n",
       "12  1036172 2            1         1          1             2           \n",
       "13  1041801 5            3         3          3             2           \n",
       "14  1043999 1            1         1          1             2           \n",
       "15  1044572 8            7         5          10            7           \n",
       "16  1047630 7            4         6          4             6           \n",
       "17  1048672 4            1         1          1             2           \n",
       "18  1049815 4            1         1          1             2           \n",
       "19  1050670 10           7         7          6             4           \n",
       "20  1050718 6            1         1          1             2           \n",
       "21  1054590 7            3         2          10            5           \n",
       "22  1054593 10           5         5          3             6           \n",
       "23  1056784 3            1         1          1             2           \n",
       "24  1057013 8            4         5          1             2           \n",
       "25  1059552 1            1         1          1             2           \n",
       "26  1065726 5            2         3          4             2           \n",
       "27  1066373 3            2         1          1             1           \n",
       "28  1066979 5            1         1          1             2           \n",
       "29  1067444 2            1         1          1             2           \n",
       "30  1070935 1            1         3          1             2           \n",
       "... ...     ...          ...       ...        ...           ...         \n",
       "670 1350423 5            10        10         8             5           \n",
       "671 1352848 3            10        7          8             5           \n",
       "672 1353092 3            2         1          2             2           \n",
       "673 1354840 2            1         1          1             2           \n",
       "674 1354840 5            3         2          1             3           \n",
       "675 1355260 1            1         1          1             2           \n",
       "676 1365075 4            1         4          1             2           \n",
       "677 1365328 1            1         2          1             2           \n",
       "678 1368267 5            1         1          1             2           \n",
       "679 1368273 1            1         1          1             2           \n",
       "680 1368882 2            1         1          1             2           \n",
       "681 1369821 10           10        10         10            5           \n",
       "682 1371026 5            10        10         10            4           \n",
       "683 1371920 5            1         1          1             2           \n",
       "684 466906  1            1         1          1             2           \n",
       "685 466906  1            1         1          1             2           \n",
       "686 534555  1            1         1          1             2           \n",
       "687 536708  1            1         1          1             2           \n",
       "688 566346  3            1         1          1             2           \n",
       "689 603148  4            1         1          1             2           \n",
       "690 654546  1            1         1          1             2           \n",
       "691 654546  1            1         1          3             2           \n",
       "692 695091  5            10        10         5             4           \n",
       "693 714039  3            1         1          1             2           \n",
       "694 763235  3            1         1          1             2           \n",
       "695 776715  3            1         1          1             3           \n",
       "696 841769  2            1         1          1             2           \n",
       "697 888820  5            10        10         3             7           \n",
       "698 897471  4            8         6          4             3           \n",
       "699 897471  4            8         8          5             4           \n",
       "    Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses Class    \n",
       "1   1           3           1               1       benign   \n",
       "2   10          3           2               1       benign   \n",
       "3   2           3           1               1       benign   \n",
       "4   4           3           7               1       benign   \n",
       "5   1           3           1               1       benign   \n",
       "6   10          9           7               1       malignant\n",
       "7   10          3           1               1       benign   \n",
       "8   1           3           1               1       benign   \n",
       "9   1           1           1               5       benign   \n",
       "10  1           2           1               1       benign   \n",
       "11  1           3           1               1       benign   \n",
       "12  1           2           1               1       benign   \n",
       "13  3           4           4               1       malignant\n",
       "14  3           3           1               1       benign   \n",
       "15  9           5           5               4       malignant\n",
       "16  1           4           3               1       malignant\n",
       "17  1           2           1               1       benign   \n",
       "18  1           3           1               1       benign   \n",
       "19  10          4           1               2       malignant\n",
       "20  1           3           1               1       benign   \n",
       "21  10          5           4               4       malignant\n",
       "22  7           7           10              1       malignant\n",
       "23  1           2           1               1       benign   \n",
       "24  NA          7           3               1       malignant\n",
       "25  1           3           1               1       benign   \n",
       "26  7           3           6               1       malignant\n",
       "27  1           2           1               1       benign   \n",
       "28  1           2           1               1       benign   \n",
       "29  1           2           1               1       benign   \n",
       "30  1           1           1               1       benign   \n",
       "... ...         ...         ...             ...     ...      \n",
       "670 5           7           10              1       malignant\n",
       "671 8           7           4               1       malignant\n",
       "672 1           3           1               1       benign   \n",
       "673 1           3           1               1       benign   \n",
       "674 1           1           1               1       benign   \n",
       "675 1           2           1               1       benign   \n",
       "676 1           1           1               1       benign   \n",
       "677 1           2           1               1       benign   \n",
       "678 1           1           1               1       benign   \n",
       "679 1           1           1               1       benign   \n",
       "680 1           1           1               1       benign   \n",
       "681 10          10          10              7       malignant\n",
       "682 10          5           6               3       malignant\n",
       "683 1           3           2               1       benign   \n",
       "684 1           1           1               1       benign   \n",
       "685 1           1           1               1       benign   \n",
       "686 1           1           1               1       benign   \n",
       "687 1           1           1               1       benign   \n",
       "688 1           2           3               1       benign   \n",
       "689 1           1           1               1       benign   \n",
       "690 1           1           1               8       benign   \n",
       "691 1           1           1               1       benign   \n",
       "692 5           4           4               1       malignant\n",
       "693 1           1           1               1       benign   \n",
       "694 1           2           1               2       benign   \n",
       "695 2           1           1               1       benign   \n",
       "696 1           1           1               1       benign   \n",
       "697 3           8           10              2       malignant\n",
       "698 4           10          6               1       malignant\n",
       "699 5           10          4               1       malignant"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading and visualizing the data\n",
    "\n",
    "data(BreastCancer)\n",
    "BreastCancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>392</li>\n",
       "\t<li>10</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 392\n",
       "\\item 10\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 392\n",
       "2. 10\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 392  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>144</li>\n",
       "\t<li>10</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 144\n",
       "\\item 10\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 144\n",
       "2. 10\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 144  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>163</li>\n",
       "\t<li>10</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 163\n",
       "\\item 10\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 163\n",
       "2. 10\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 163  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adjusting data types \n",
    "data <- BreastCancer[ , -1] # remove the ID column \n",
    "data[ , c(1: ncol(data))] <- sapply(data[ , c(1: ncol(data))], as.numeric)\n",
    "data[ ,'Class'] <- as.factor(data[ ,'Class'])\n",
    "\n",
    "# converting in the h2o format \n",
    "splitSample  <- sample(1:3, size = nrow(data), prob = c(0.6, 0.2, 0.2), replace = TRUE )\n",
    "\n",
    "train_h2o <- as.h2o(data[splitSample ==1 , ])\n",
    "valid_h2o <- as.h2o(data[splitSample ==2 , ])\n",
    "test_h2o <- as.h2o(data[splitSample ==3 , ])\n",
    "\n",
    "# printing dimentions \n",
    "dim(train_h2o)\n",
    "dim(valid_h2o)\n",
    "dim(test_h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can train our Deep Learning Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "dl_model <- h2o.deeplearning(\n",
    "                x = 1:9, # column numbers for predictors\n",
    "                y = 10,  # column numbers for labels\n",
    "                training_frame = train_h2o, # Data in H2O formate\n",
    "                activation = \"TanhWithDropout\",\n",
    "                input_dropout_ratio = 0.2, # % of inputs dropout\n",
    "                balance_classes = TRUE,\n",
    "                hidden = c(10,10), # two hidden layers of the 10 units\n",
    "                hidden_dropout_ratios = c(0.3, 0.3), # % for nodes dropouts\n",
    "                epochs = 10, # max. no. of epochs\n",
    "                seed = 0    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results on the train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>1</th><th scope=col>2</th><th scope=col>Error</th><th scope=col>Rate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>257     </td><td>  9     </td><td>0.03383 </td><td> =9/266 </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>  2     </td><td>266     </td><td>0.00746 </td><td> =2/268 </td></tr>\n",
       "\t<tr><th scope=row>Totals</th><td>259     </td><td>275     </td><td>0.02060 </td><td> =11/534</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & 1 & 2 & Error & Rate\\\\\n",
       "\\hline\n",
       "\t1 & 257      &   9      & 0.03383  &  =9/266 \\\\\n",
       "\t2 &   2      & 266      & 0.00746  &  =2/268 \\\\\n",
       "\tTotals & 259      & 275      & 0.02060  &  =11/534\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 1 | 2 | Error | Rate | \n",
       "|---|---|---|\n",
       "| 1 | 257      |   9      | 0.03383  |  =9/266  | \n",
       "| 2 |   2      | 266      | 0.00746  |  =2/268  | \n",
       "| Totals | 259      | 275      | 0.02060  |  =11/534 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       1   2   Error   Rate    \n",
       "1      257   9 0.03383  =9/266 \n",
       "2        2 266 0.00746  =2/268 \n",
       "Totals 259 275 0.02060  =11/534"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training confusion matrix\n",
    "h2o.confusionMatrix(dl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>1</th><th scope=col>2</th><th scope=col>Error</th><th scope=col>Rate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>90     </td><td> 3     </td><td>0.0323 </td><td> =3/93 </td></tr>\n",
       "\t<tr><th scope=row>2</th><td> 0     </td><td>51     </td><td>0.0000 </td><td> =0/51 </td></tr>\n",
       "\t<tr><th scope=row>Totals</th><td>90     </td><td>54     </td><td>0.0208 </td><td> =3/144</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & 1 & 2 & Error & Rate\\\\\n",
       "\\hline\n",
       "\t1 & 90      &  3      & 0.0323  &  =3/93 \\\\\n",
       "\t2 &  0      & 51      & 0.0000  &  =0/51 \\\\\n",
       "\tTotals & 90      & 54      & 0.0208  &  =3/144\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 1 | 2 | Error | Rate | \n",
       "|---|---|---|\n",
       "| 1 | 90      |  3      | 0.0323  |  =3/93  | \n",
       "| 2 |  0      | 51      | 0.0000  |  =0/51  | \n",
       "| Totals | 90      | 54      | 0.0208  |  =3/144 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       1  2  Error  Rate   \n",
       "1      90  3 0.0323  =3/93 \n",
       "2       0 51 0.0000  =0/51 \n",
       "Totals 90 54 0.0208  =3/144"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validation confusion matrix\n",
    "h2o.confusionMatrix(dl_model, valid_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>1</th><th scope=col>2</th><th scope=col>Error</th><th scope=col>Rate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>93     </td><td> 6     </td><td>0.0606 </td><td> =6/99 </td></tr>\n",
       "\t<tr><th scope=row>2</th><td> 0     </td><td>64     </td><td>0.0000 </td><td> =0/64 </td></tr>\n",
       "\t<tr><th scope=row>Totals</th><td>93     </td><td>70     </td><td>0.0368 </td><td> =6/163</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & 1 & 2 & Error & Rate\\\\\n",
       "\\hline\n",
       "\t1 & 93      &  6      & 0.0606  &  =6/99 \\\\\n",
       "\t2 &  0      & 64      & 0.0000  &  =0/64 \\\\\n",
       "\tTotals & 93      & 70      & 0.0368  &  =6/163\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 1 | 2 | Error | Rate | \n",
       "|---|---|---|\n",
       "| 1 | 93      |  6      | 0.0606  |  =6/99  | \n",
       "| 2 |  0      | 64      | 0.0000  |  =0/64  | \n",
       "| Totals | 93      | 70      | 0.0368  |  =6/163 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       1  2  Error  Rate   \n",
       "1      93  6 0.0606  =6/99 \n",
       "2       0 64 0.0000  =0/64 \n",
       "Totals 93 70 0.0368  =6/163"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test confusion matrix\n",
    "h2o.confusionMatrix(dl_model, test_h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "+ Multi-hidden-layer ANNs \n",
    "+ Explored the H20 package and its setup \n",
    "+ Got more confident in solving real-world problems with ANNs and R \n",
    "+ Explained about tuning the parameters and best practices for the deployment of ANNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning as an Optimization Process \n",
    "\n",
    "+ What do we mean by Learning \n",
    "    + Being able to generalize from previous experiences and improve the performances on the same task based on a given measure \n",
    "    + More specifically, it means adjusting the parameters of the model in order to accurately predict dependent variables on new input data\n",
    "    \n",
    "### The Score Function and the Loss Function\n",
    "\n",
    "More formally we can define two main functions:\n",
    "\n",
    "+ **The Score Function** f(0), which describes our mapping from the input space x to the output space y \n",
    "+ **The Loss function** J(0), which measures the quality of a particular set of parameters based on how well the induced scores (using f(0)) agreed with the ground truth labels in the training data\n",
    "\n",
    "Having chosen the score function and the loss function, we just need to adjust them in order to **minimize the Loss Function** \n",
    "\n",
    "+ We can initialize them randomly \n",
    "+ Then we can use the best optimization algorithm available\n",
    "\n",
    "### Designing the Loss Function\n",
    "\n",
    "+ As for the choice of the loss function many different loss functions can be designed for the same problem \n",
    "+ An example of loss function for the regression problem could be: \n",
    "\n",
    "<img src=\"./resources/lossfunction.png\" alt=\"lossfunction\" height=\"25%\" width=\"25%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The MSE can be used also for classification problems\n",
    "+ Other common Loss functions are: \n",
    "    + Cross-Entropy Loss Function \n",
    "    + Hinge Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Regularization Term\n",
    "\n",
    "In the previous section we have also seen how to prevent overfitting using regularization terms \n",
    "\n",
    "+ Remember to always add them to the Loss Function\n",
    "+ Continuing our example with the MSE-based loss function, using the L2 regularization we would have: \n",
    "\n",
    "<img src=\"./resources/lossfunction1.png\" alt=\"lossfunction1\" height=\"30%\" width=\"30%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Optimization Algorithms \n",
    "\n",
    "In mathematics, computer science and operations research, mathematical optimization is the selection of a best element(s) \n",
    "(with regards to some criterion) from some set of available alternatives.\n",
    "\n",
    "More formally: \n",
    "\n",
    "+ ***Given***: A function **f: A -> R** from some set A to the real numbers  \n",
    "+ ***Sought***: An element xo in A such that **f(xo) <= f(x)** for all x in A    ///(\"minimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ An optimization process could be exact or approximate, which means it cannot guarantee to find the optimal solution \n",
    "+ Function surface, an approximate optimization algorithm is generally preferred in the context of Deep Learning\n",
    "\n",
    "<img src=\"./resources/optimizationplot.png\" alt=\"optimizationplot\" height=\"40%\" width=\"40%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search \n",
    "\n",
    "+ What would be the easiest way to minimize a function (given unlimited computational power and time) \n",
    "+ Easy! We can just try all the possible combination of values in the domain of the parameters and choose the set that corresponds to the minimum value of the loss function\n",
    "    \n",
    "    + Since in the real world we don't have unlimited resources, in the Random Search approach the value of the parameters is chosen randomly and the loss function is evaluated\n",
    "    + We repeat this operation as many times we want and just keep track of the set of parameters that minimize the loss function\n",
    "\n",
    "### Random Local Search\n",
    "\n",
    "+ Random Search is never used in practice, especially if the parameter space is very large like in Deep Learning models \n",
    "+ A second approach we may think of is the Random Local Search: \n",
    "    + In this approach, instead of trying many random sets of parameters, we can start with **a random solution 0** and than update such parameters with a random perturbation only if it leads to a better solution \n",
    "    + With this solution we do not start from scratch at each iteration but we try to improve the current solution\n",
    "\n",
    "### Gradient Descent and Stochastic Gradient Descent (SGD) \n",
    "\n",
    "+ Even though Random Local Search is better than Random Search, it is still wasteful and computationally expensive \n",
    "+ It turns out that there is no need to randomly search for a good direction: \n",
    "    + Compute the best direction along which we should change our parameters that is mathematically guaranteed to be the direction of the steepest descend (towards the local minimum of the function) \n",
    "    + This direction will be related to the gradient of the loss function\n",
    "+ But... What is the gradientof a function \n",
    "    + In one-dimensional functions, the derivative(s/ope) is the instantaneous rate of change of the function at any point you might be interested in.\n",
    "    + When the functions of interest is multi-dimensional, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension\n",
    "\n",
    "<img src=\"./resources/gradient.png\" alt=\"gradient\" height=\"20%\" width=\"20%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/partialderivative.png\" alt=\"partialderivative\" height=\"20%\" width=\"20%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/minima.png\" alt=\"minima\" height=\"30%\" width=\"30%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/minima3D.png\" alt=\"minima3D\" height=\"30%\" width=\"30%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In large-scale applications the training set can have on order of millions of examples :\n",
    "\n",
    "+ Hence, it seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update \n",
    "+ A very common approach to addressing this problem is to compute the gradient over small batches of the training set but the introduction of a bit of \n",
    "+ This is not only more efficient randomness (That is, not choosing the best local direction towards the minimum) can also be beneficial to iumo off small local minima\n",
    "\n",
    "This approach is called **Stochastic Gradient Descent (SGD)**, or **Mini-batch Gradient Descent (MGD)**\n",
    "\n",
    "+ It is possible to vary the dimension of each mini-batch such that in the extreme cases we end-up with. \n",
    "    + Online Gradient Descent (mini-batches of one example) \n",
    "    + Classical Gradient Descent (one big batch containing the entire training set) \n",
    "+ Setting the mini-batch size to 128 or 256 is the most common choice\n",
    "\n",
    "### Introduction to Backpropagation\n",
    "\n",
    "+ Backpropagation (backward propagation of errors) is a way of computing gradients of expressions through recursive app/icationof chain rule \n",
    "    + It is the standard way of computing gradients for ANNs \n",
    "    + Introduced in the context of ANNs in 1986 by David Rumelhart, Geoffrey Hinton, and Ronald Williams \n",
    "    + Flexible and general solution for computing the gradient through simple, incremental steps\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "In calculus, the chain rule is a formula for computing the derivative of the composition of two or more functions \n",
    "\n",
    "+ Let's say that we want to compute **df/dx** where f is: **f(x,y,z)= (x+y) * z**\n",
    "+ This expression can be broken down into two expressions: **q = x+y , f= q * z**\n",
    "+ Waht the chain rules says is that we can compute : **df/dx = (df/dq) * (dq/dx)**\n",
    "+ It says we can compute the derivative of f with respect to x, based on product between the derivative of q with respect to x, and the derivative of f with respect to q \n",
    "+ The Chain Rule, allow us to break down a complex function in many smaller functions for which it is relatively straightforward to compute the derivative. Then we simply chain them together by multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Weights of the Perceptron \n",
    "\n",
    "<img src=\"./resources/ann.png\" alt=\"ann\" height=\"35%\" width=\"35%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/lossfunction2.png\" alt=\"lossfunction2\" height=\"30%\" width=\"30%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/lossfunction3.png\" alt=\"lossfunction3\" height=\"60%\" width=\"60%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the gradient of the Loss Function with respect to all the weights we can update our model simply by: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/weights.png\" alt=\"weights\" height=\"20%\" width=\"20%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ For simplicity, here, we have not considered the regularization term inside the loss function, \n",
    "+ But remember to always take it into account, especially for larger models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation \n",
    "\n",
    "Luckily, we don't have to go through all this math each time we want to develop a new Deep Learning model \n",
    "\n",
    "+ Use fully-fledged DL packages as we already did to solve our real-world use-cases with pre-defined Loss-Function \n",
    "+ Use automatic differentiation packages to provide just the Loss Function and let them compute the gradient for us. \n",
    "+ This is generally the best strategy to work at a lower level of abstraction and gain more flexibility \n",
    "\n",
    "### Manual Versus Automatic hyper-parameters selection \n",
    "\n",
    "+ Until now, we have only considered tuning the hyper-parameters of our models manually \n",
    "+ Nevertheless, we have already learned about SGD, a very general optimization algorithm \n",
    "+ Why not applying it to learn the hyper-parameters too \n",
    "+ In general, evaluating the Loss Function with respect to the hyper-parameters means training the model from scratch each time which makes the problem computationally impossible to solve \n",
    "+ Despite being a very active field of research, nowadays it is pretty common to tune the hyper-parameters manually with some tricks and experience \n",
    "+ For small models, some techniques have show empirically to be able to find a better hyper-parametrization than a ML expert:\n",
    "    + Grid Search \n",
    "    + Random-search \n",
    "    + Bayesian Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search and Random Search\n",
    "\n",
    "<img src=\"./resources/hyperparameters.png\" alt=\"hyperparameters\" height=\"40%\" width=\"40%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters Optimization in R with H2O \n",
    "\n",
    "+ More practically we can implement a random search in R using H20 which comes with a complete set of utility functions specifically designed for this purpose \n",
    "+ The h2o.grid() function: \n",
    "    + Very general and adaptive \n",
    "    + Efficient and Distributed implementation of Grid or Random search \n",
    "    + Few lines of R code for a complex hyper-parameters selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll learn how to choose the best hyper-prameters of a given model using the h2o.grid() method. \n",
    "Firstly, let's recover the code we implemented in the previous video for using the Breast-Cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "library(mlbench)\n",
    "library(h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Connection successful!\n",
      "\n",
      "R is connected to the H2O cluster: \n",
      "    H2O cluster uptime:         1 minutes 17 seconds \n",
      "    H2O cluster version:        3.16.0.2 \n",
      "    H2O cluster version age:    2 months and 3 days  \n",
      "    H2O cluster name:           H2O_started_from_R_Sarfaraz_qiy213 \n",
      "    H2O cluster total nodes:    1 \n",
      "    H2O cluster total memory:   0.78 GB \n",
      "    H2O cluster total cores:    4 \n",
      "    H2O cluster allowed cores:  4 \n",
      "    H2O cluster healthy:        TRUE \n",
      "    H2O Connection ip:          localhost \n",
      "    H2O Connection port:        54321 \n",
      "    H2O Connection proxy:       NA \n",
      "    H2O Internal Security:      FALSE \n",
      "    H2O API Extensions:         Algos, AutoML, Core V3, Core V4 \n",
      "    R Version:                  R version 3.4.2 (2017-09-28) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# general parameters\n",
    "h2o.init()\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "data(BreastCancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data types\n",
    "\n",
    "data[ , c(1:ncol(data))] <- sapply(data[ , c(1:ncol(data))], as.numeric )\n",
    "data[ , 'Class'] <- as.factor(data[ , 'Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset and converting to h2o format\n",
    "\n",
    "splitSample <- sample(1:3, size = nrow(data), prob=c(0.7,0.15,0.15), replace = TRUE)\n",
    "train_h2o <- as.h2o(data[splitSample==1, ])\n",
    "valid_h2o <- as.h2o(data[splitSample==2, ])\n",
    "test_h2o <- as.h2o(data[splitSample==3, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can choose the best hyper-parameters using the random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the hyper-parameters to test\n",
    "\n",
    "hyper_params <- list(\n",
    "    activation = c('Rectifier', 'Tanh', 'Maxout', 'RectifierWithDropout', 'MaxoutWithDropout'),\n",
    "    hidden = list(c(20,20), c(50,50), c(30,30,30), c(25,25,25)),\n",
    "    input_dropout_ratio = c(0, 0.05),\n",
    "    l1 = seq(0, 1e-4,1e-6),\n",
    "    l2 = seq(0, 1e-4,1e-6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the searching criteria\n",
    "\n",
    "search_criteria = list(\n",
    "    strategy = \"RandomDiscrete\",\n",
    "    max_runtime_secs = 360,\n",
    "    max_models =100,\n",
    "    seed = 1234567,\n",
    "    stopping_rounds = 5,\n",
    "    stopping_tolerance = 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  |                                                                            \r",
      "  |                                                                      |   0%\r",
      "  |                                                                            \r",
      "  |=                                                                     |   2%\r",
      "  |                                                                            \r",
      "  |=====                                                                 |   7%\r",
      "  |                                                                            \r",
      "  |==========                                                            |  15%\r",
      "  |                                                                            \r",
      "  |=================                                                     |  25%\r",
      "  |                                                                            \r",
      "  |========================                                              |  34%\r",
      "  |                                                                            \r",
      "  |===============================                                       |  45%\r",
      "  |                                                                            \r",
      "  |=======================================                               |  55%\r",
      "  |                                                                            \r",
      "  |==========================================                            |  60%\r",
      "  |                                                                            \r",
      "  |==================================================                    |  71%\r",
      "  |                                                                            \r",
      "  |=======================================================               |  78%\r",
      "  |                                                                            \r",
      "  |===========================================================           |  85%\r",
      "  |                                                                            \r",
      "  |======================================================================|  99%\r",
      "  |                                                                            \r",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "# Using the h2o.grid function\n",
    "\n",
    "dl_random_grid <- h2o.grid(\n",
    "    algorithm = 'deeplearning',\n",
    "    grid_id = 'dl_grid_random',\n",
    "    training_frame = train_h2o,\n",
    "    validation_frame = valid_h2o,\n",
    "    x = 1:9,\n",
    "    y = 10,\n",
    "    epochs = 1,\n",
    "    ## stop when logloss doesn't improve by >= 1% for 2 scoring events\n",
    "    stopping_metric = \"logloss\",\n",
    "    stopping_tolerance = 1e-2,\n",
    "    stopping_rounds = 2,\n",
    "    hyper_params = hyper_params,\n",
    "    search_criteria = search_criteria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>activation</th><th scope=col>hidden</th><th scope=col>input_dropout_ratio</th><th scope=col>l1</th><th scope=col>l2</th><th scope=col>model_ids</th><th scope=col>logloss</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Maxout                </td><td>[30, 30, 30]          </td><td>0.05                  </td><td>7.0E-5                </td><td>8.2E-5                </td><td>dl_grid_random_model_1</td><td>0.06347894122796625   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " activation & hidden & input\\_dropout\\_ratio & l1 & l2 & model\\_ids & logloss\\\\\n",
       "\\hline\n",
       "\t Maxout                         & {[}30, 30, 30{]}               & 0.05                           & 7.0E-5                         & 8.2E-5                         & dl\\_grid\\_random\\_model\\_1 & 0.06347894122796625           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "activation | hidden | input_dropout_ratio | l1 | l2 | model_ids | logloss | \n",
       "|---|\n",
       "| Maxout                 | [30, 30, 30]           | 0.05                   | 7.0E-5                 | 8.2E-5                 | dl_grid_random_model_1 | 0.06347894122796625    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  activation hidden       input_dropout_ratio l1     l2    \n",
       "1 Maxout     [30, 30, 30] 0.05                7.0E-5 8.2E-5\n",
       "  model_ids              logloss            \n",
       "1 dl_grid_random_model_1 0.06347894122796625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# recover the grid\n",
    "grid <- h2o.getGrid('dl_grid_random', sort_by = 'logloss', decreasing = FALSE)\n",
    "grid@summary_table[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Details:\n",
       "==============\n",
       "\n",
       "H2OBinomialModel: deeplearning\n",
       "Model ID:  dl_grid_random_model_1 \n",
       "Status of Neuron Layers: predicting Class, 2-class classification, bernoulli distribution, CrossEntropy loss, 4,382 weights/biases, 58.8 KB, 522 training samples, mini-batch size 1\n",
       "  layer units    type dropout       l1       l2 mean_rate rate_rms momentum\n",
       "1     1     9   Input  5.00 %                                              \n",
       "2     2    30  Maxout  0.00 % 0.000070 0.000082  0.002031 0.002154 0.000000\n",
       "3     3    30  Maxout  0.00 % 0.000070 0.000082  0.164904 0.374610 0.000000\n",
       "4     4    30  Maxout  0.00 % 0.000070 0.000082  0.421297 0.485399 0.000000\n",
       "5     5     2 Softmax         0.000070 0.000082  0.001207 0.000844 0.000000\n",
       "  mean_weight weight_rms mean_bias bias_rms\n",
       "1                                          \n",
       "2    0.010146   0.229974  0.500072 0.032290\n",
       "3   -0.000213   0.189135  0.998532 0.011892\n",
       "4    0.001677   0.187767  0.999560 0.002582\n",
       "5    0.059534   1.052306 -0.000201 0.001691\n",
       "\n",
       "\n",
       "H2OBinomialMetrics: deeplearning\n",
       "** Reported on training data. **\n",
       "** Metrics reported on full training frame **\n",
       "\n",
       "MSE:  0.023\n",
       "RMSE:  0.152\n",
       "LogLoss:  0.0911\n",
       "Mean Per-Class Error:  0.0198\n",
       "AUC:  0.996\n",
       "Gini:  0.991\n",
       "\n",
       "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
       "         1   2    Error     Rate\n",
       "1      328   9 0.026706   =9/337\n",
       "2        2 153 0.012903   =2/155\n",
       "Totals 330 162 0.022358  =11/492\n",
       "\n",
       "Maximum Metrics: Maximum metrics at their respective thresholds\n",
       "                        metric threshold    value idx\n",
       "1                       max f1  0.666232 0.965300 146\n",
       "2                       max f2  0.130350 0.983503 152\n",
       "3                 max f0point5  0.666232 0.952677 146\n",
       "4                 max accuracy  0.666232 0.977642 146\n",
       "5                max precision  1.000000 1.000000   0\n",
       "6                   max recall  0.130350 1.000000 152\n",
       "7              max specificity  1.000000 1.000000   0\n",
       "8             max absolute_mcc  0.666232 0.949323 146\n",
       "9   max min_per_class_accuracy  0.779544 0.973294 144\n",
       "10 max mean_per_class_accuracy  0.130350 0.980712 152\n",
       "\n",
       "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
       "H2OBinomialMetrics: deeplearning\n",
       "** Reported on validation data. **\n",
       "** Metrics reported on full validation frame **\n",
       "\n",
       "MSE:  0.0207\n",
       "RMSE:  0.144\n",
       "LogLoss:  0.0635\n",
       "Mean Per-Class Error:  0.0203\n",
       "AUC:  0.998\n",
       "Gini:  0.997\n",
       "\n",
       "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
       "        1  2    Error    Rate\n",
       "1      63  1 0.015625   =1/64\n",
       "2       1 39 0.025000   =1/40\n",
       "Totals 64 40 0.019231  =2/104\n",
       "\n",
       "Maximum Metrics: Maximum metrics at their respective thresholds\n",
       "                        metric threshold    value idx\n",
       "1                       max f1  0.841509 0.975000  39\n",
       "2                       max f2  0.562121 0.985222  42\n",
       "3                 max f0point5  0.960528 0.989583  37\n",
       "4                 max accuracy  0.960528 0.980769  37\n",
       "5                max precision  1.000000 1.000000   0\n",
       "6                   max recall  0.562121 1.000000  42\n",
       "7              max specificity  1.000000 1.000000   0\n",
       "8             max absolute_mcc  0.960528 0.959798  37\n",
       "9   max min_per_class_accuracy  0.841509 0.975000  39\n",
       "10 max mean_per_class_accuracy  0.841509 0.979688  39\n",
       "\n",
       "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take the best model with lowest logloss\n",
    "\n",
    "best_model <- h2o.getModel(grid@model_ids[[1]])\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "+ Explored basic concepts of the learning process \n",
    "+ Introduced optimization algorithms and SGD \n",
    "+ Discussed backpropagation in detail: How you can train an ANN from scratch \n",
    "+ Hyper-parameters Optimization in R with H20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks — Key Features \n",
    "\n",
    "As we saw in the previous sections, Artificial Neural Networks receive an input (a single vector), and transform it through a series of hidden layers: \n",
    "\n",
    "+ Let's say we have to classify 32 x 32 RGB images. A single fully-connected neuron in a first hidden layer would have 3072 weights \n",
    "+ This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images \n",
    "+ For example, an image of more respectable size, for example, 200 x 200, would lead to neurons that have 120,000 weights \n",
    "+ Key Features: \n",
    "    + Still made up of neurons that have learnable weights and biases \n",
    "    + Weights Sharing and Local Connectivity \n",
    "+ This means:\n",
    "    + We are able to scale very well in terms of number of neurons \n",
    "    + They acquire translation invariance an robustness with regards to noise \n",
    "    + They work extremely well on high-dimentional patterns like images \n",
    "\n",
    "<img src=\"./resources/cnn.png\" alt=\"cnn\" height=\"40%\" width=\"40%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Overview \n",
    "\n",
    "Concerning the architecture: \n",
    "\n",
    "+ A CNN is composed of a number of subsequent layers, where each layer has neurons arranged in 3 dimensions: width, height, depth \n",
    "+ For example, an input image can be considered as an input volume of activations where the depth is 3 as the different color channels \n",
    "\n",
    "<img src=\"./resources/cnn1.png\" alt=\"cnn\" height=\"40%\" width=\"40%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We can see a CNN also as a sequence of layers, where each layer transforms one volume of activations to another through a differentiable function \n",
    "+ There are three fundamental types of layers: \n",
    "    + Convolutional Layer \n",
    "    + Pooling Layer \n",
    "    + Fully-Connected Layer \n",
    "\n",
    "### Convolutional Layers \n",
    "\n",
    "+ In order to understand Convolutional Layers, first we need to introduce convolution operation \n",
    "+ In Computer Vision, a very typical approach for processing an Image is to convolve it with a filter(or kernel) in order to extract only salient features from the image \n",
    "\n",
    "<img src=\"./resources/cnn2.png\" alt=\"cnn\" height=\"40%\" width=\"40%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/cnn3.png\" alt=\"cnn\" height=\"40%\" width=\"40%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to introduce a bit of terminology: \n",
    "\n",
    "+ With Feature Maps, we mean each slide of the conv. layer along the width \n",
    "+ With Receptive Field, we mean the local 3D-patch in the input volume to which a specific neuron is connected \n",
    "+ With Depth Column, we refer to a set of neurons that are all looking at the same region of the input \n",
    "\n",
    "<img src=\"./resources/cnn4.png\" alt=\"cnn\" height=\"20%\" width=\"20%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Convolutional layers, it's like performing a 3D \n",
    "\n",
    "+ All the input volume is convolved using different neurons \n",
    "+ Each neuron is connected only to its local receptive-field (filter-size * depth input neurons) and is like having a 3-dimensional filter \n",
    "+ Each neuron in each output feature map shares the same weights (it has the same filter) \n",
    "\n",
    "We need more than one output feature map: \n",
    "\n",
    "+ Because each filter can extract different features from the input image \n",
    "+ The number of feature maps for each convolutional layer would be the another hyper-parameterto tune\n",
    "\n",
    "### Pooling Layers\n",
    "\n",
    "+ It is common to periodically insert a Pooling layer in-between successive convolution layers in a CNN architecture \n",
    "+ Its function is to reduce the amount of parameters and computation in the network and improve robustness \n",
    "+ The pooling is simply performed independently on each feature map\n",
    "\n",
    "<img src=\"./resources/poolinglayer.png\" alt=\"poolinglayer\" height=\"30%\" width=\"30%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/poolinglayer1.png\" alt=\"poolinglayer\" height=\"30%\" width=\"30%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layers\n",
    "\n",
    "+ Neurons in a fully-connected layer have full connections to all activations in the previous layer, as seen in regular Artificial Neural Networks \n",
    "+ Their activations can hence be computed with a matrix multiplication followed by a bias offset \n",
    "+ If the previous layer has a 3-dimentional volume you can think about it as reshaped in a single long input vector\n",
    "\n",
    "<img src=\"./resources/fcl.png\" alt=\"Fully-Connected Layers\" height=\"30%\" width=\"30%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Design the Architecture? \n",
    "\n",
    "+ The most common form of a CNN architecture stacks a few CONV layers followed by RELU activation functions and a POOL layer \n",
    "+ This pattern is repeated until the image has been merged spatially into a small size \n",
    "+ At this point, it is common to transition to fully-connected layers, the last fully-connected layer holds the output, such as the class scores \n",
    "\n",
    "<img src=\"./resources/fcl1.png\" alt=\"Fully-Connected Layers\" height=\"30%\" width=\"30%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the most common CNN architecture follows the pattern: \n",
    "\n",
    "`INPUT  -> [[CONV -> RELU] * N -> POOL] * M -> [FC -> RELU] * K -> FC`\n",
    " \n",
    "Where: \n",
    "+ \\* indicates repetition \n",
    "+ N >= 0 usually N <= 3 \n",
    "+ M >= 0\n",
    "+ K >= 0 usually K <= 2\n",
    "\n",
    "An example of CNN could be: \n",
    "\n",
    "`INPUT  -> [[CONV -> RELU -> CONV -> RELU -> POOL] * 3 -> [FC -> RELU] * 2 -> FC`\n",
    "\n",
    "Other tips for deigning a CNN:\n",
    "+ Prefer a stack of small filter CONV to one large receptive field CONV layer \n",
    "+ The most common CONV layer filter sizes are 3x3 and 5x5 \n",
    "+ The most common POOL layer filter size is 2x2 \n",
    "+ Use the best compromise between accuracy and hardware resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common CNNs Architectures \n",
    "There are several architectures in the field of Convolutional Networks that have a name, the most common are: \n",
    "1. LeNet \n",
    "    + The first successful applications of Convolutional Networks were developed by Y \n",
    "    + LeCun in 1990's. Of these, the best known is the LeNet architecture that was used to read zip codes, digits, and so on. \n",
    "2. AlexNet\n",
    "    + The first work that popularized Convolutional Networks in Computer Vision was the AlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton \n",
    "\n",
    "3. VGGNet \n",
    "    + Its main contribution was in showing that the depth of the network is a critical component for good performance \n",
    "    + Their final best network contains 16 CONV/FC layers and is composed by an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding and Visualizing CNNs \n",
    "+ Convolutional Neural Networks have had a tremendous impact on the computer vision, pattern recognition, and machine learning community \n",
    "+ Despite their success, they are often referred as a blaclebox algorithm in which you can't really tell what is going on under the hood \n",
    "+ Indeed, once the training is done and we can't classify an image correctly can we answer to this simple question: why?? \n",
    "+ To face this problem several approaches for understanding and visualizing Convolutional Networks have been developed in the literature \n",
    "\n",
    "<img src=\"./resources/visualcnn.png\" alt=\"visualcnn\" height=\"50%\" width=\"50%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/visualcnn1.png\" alt=\"visualcnn\" height=\"50%\" width=\"50%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/visualcnn2.png\" alt=\"visualcnn\" height=\"60%\" width=\"60%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Pre-Trained Models and Transfer Learning \n",
    "\n",
    "+ In practice, very few people train an entire CNN from scratch because it is relatively rare to have a dataset of sufficient size and the training can take even weeks \n",
    "+ With Transfer Learning, we mean the possibility of using a model trained on a specific task and adapt it to solve a new one actually transferring its knowledge \n",
    "    + In fact, it is common to pre-trained a CNN on a very large dataset (For example: ImageNet, which contains 1.2 million images with 1000 categories) \n",
    "    + Then use the CNN either as an initialization or a fixed feature extractor for the task of interest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the CNN as a Fixed Features Extractor \n",
    "The most common and easiest approach is to use the CNN as a fixed feature extractor. In this case:\n",
    "+ Take a pre-trained CNN on ImageNet \n",
    "+ Remove the last fully-connected layer (this layer's outputs are the 1000 class scores for a different task like ImageNet) \n",
    "+ Then treat the rest of the CNN as a fixed feature extractor for the new dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common and easiest approach is to use the CNN as a fixed feature extractor. In this case:\n",
    "+ We call these features CNN codes \n",
    "+ It is important for performance that these codes are ReLUd if they were also thresholded during the training of the CNN on ImageNet \n",
    "+ Once you extract the 4096-D codes for all images, train a linear classifier (For example, Linear SVM or A small ANN) for the new dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning the CNN \n",
    "The second strategy is not only to replace and retrain the classifier on top of the CNN on the new dataset, but to also fine-tune the weights of the pre-trained network by continuing the backpropagation \n",
    "+ It is possible to fine-tune all the layers of the CNN, or just some of them \n",
    "+ This is motivated by the observation that the earlier features of a CNN contain more generic features, but later layers of the CNN becomes progressively more specific to the classes contained in the original dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pre-Trained CNN for Classifying Real-World Images \n",
    "Let's say we are an independent company who needs to semantically understand the image content a particular user is uploading on our site \n",
    "+ We don't have a huge training set, or we don't have it at all \n",
    "+ We don't have the computational resources to train a huge CNN for a week (even though Amazon Elastic Compute Cloud is great if you want to borrow some resources) \n",
    "+ We want a quick and dirty solution to understand if the new feature is worth integrating in our system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn how to classify real-world images with pre-defined models\n",
    "\n",
    "+ Download the required files from [here](http://data.dmlc.ml/mxnet/models/imagenet/inception-bn_old.tar.gz) (41 MB).\n",
    "+ let's import the packages we'll use and recover the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in loadNamespace(name): there is no package called 'devtools'\n",
     "output_type": "error",
     "traceback": [
      "Error in loadNamespace(name): there is no package called 'devtools'\nTraceback:\n",
      "1. devtools::install_github",
      "2. getExportedValue(pkg, name)",
      "3. asNamespace(ns)",
      "4. getNamespace(ns)",
      "5. tryCatch(loadNamespace(name), error = function(e) stop(e))",
      "6. tryCatchList(expr, classes, parentenv, handlers)",
      "7. tryCatchOne(expr, names, parentenv, handlers[[1L]])",
      "8. value[[3L]](cond)"
     ]
    }
   ],
   "source": [
    "devtools::install_github(\"dahtah/imager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'devtools' is not available (for R version 3.4.2)\""
     ]
    }
   ],
   "source": [
    "install.packages(\"devtools\", repos = \"http://cran.us.r-project.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n",
     "output_type": "error",
     "traceback": [
      "Error in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\nTraceback:\n",
      "1. install.packages(\"mxnet\", repos = cran)",
      "2. contrib.url(repos, \"source\")",
      "3. stop(\"trying to use CRAN without setting a mirror\")"
     ]
    }
   ],
   "source": [
    "cran <- getOption(\"repos\")\n",
    "cran[\"dmlc\"] <- \"https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/\"\n",
    "options(repos = cran)\n",
    "install.packages(\"mxnet\", repos = \"http://cran.us.r-project.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(mxnet): there is no package called 'mxnet'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(mxnet): there is no package called 'mxnet'\nTraceback:\n",
      "1. library(mxnet)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "library(mxnet)\n",
    "a <- mx.nd.ones(c(2,3), ctx = mx.cpu())\n",
    "b <- a * 2 + 1\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: mxnet\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'mxnet'\"Loading required package: imager\n",
      "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
      "\"there is no package called 'imager'\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in mx.model.load(\"./inception/Inception_BN\", iteration = 39): could not find function \"mx.model.load\"\n",
     "output_type": "error",
     "traceback": [
      "Error in mx.model.load(\"./inception/Inception_BN\", iteration = 39): could not find function \"mx.model.load\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "require(mxnet)\n",
    "require(imager)\n",
    "\n",
    "model = mx.model.load(\"./inception/Inception_BN\", iteration = 39)\n",
    "mean.img = as.array(mx.nd.load(\"./inception/mean_224.nd\")[[\"mean_img\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's load and show the image we want to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im <- load.image(\"./resources/animal.jpg\")\n",
    "plot(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define the function to preprocess the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc.image <- function (im, mean.image) {\n",
    "    # Crop the image\n",
    "    shape <- dim (im)\n",
    "    short.edge <- min(shape[1:2])\n",
    "    yy <- floor((shape[1] - short.edge) / 2) + 1\n",
    "    yend <- yy + short.edge - 1\n",
    "    xx <- floor((shape[2] - short.edge) / 2) + 1\n",
    "    xend <- xx + short.edge - 1\n",
    "    \n",
    "    cropped <- as.cimg (im[yy:yend, xx:xend, ,])\n",
    "    \n",
    "    # resize to 224 x 224, needed by input of the model\n",
    "    resized <- imager::resize(cropped, 224, 224)\n",
    "    \n",
    "    # convert to array (x, y, channel)\n",
    "    arr <- as.array(resized)\n",
    "    dim(arr) = c(224, 224, 3)\n",
    "    \n",
    "    # substract the mean\n",
    "    normed <- arr*255 - mean.image\n",
    "    \n",
    "    # Reshape to format needed by mxnet (width, height, channel, num)\n",
    "    dim(normed) <- c(224, 224, 3, 1)\n",
    "    return(normed)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get the probabilities for each class and take the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed <- preproc.image(im, mean.img)\n",
    "probs <- predict(model, X = normed)\n",
    "max.idx <- order(probs, decreasing = TRUE)[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply print the class label for the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets <- readLines(\"./inception/synset.txt\")\n",
    "print(paste0(\"Predicted Top-classes: \"),synsets[max.idx]))\n",
    "probs[max.idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "+ Discussed what is a CNN and what it is made of \n",
    "+ Explored how to design a CNN architecture suitable for your task \n",
    "+ Implemented a CNN in R with the MXNet package \n",
    "+ Solved a real-world problem in the blink of an eye with pre-trained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
